{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b461df8a-b3d3-4d6e-ac8d-75d05f278c46",
   "metadata": {},
   "source": [
    "# Claude programming code responses test for diabetes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4427503-eddd-48da-bcd4-0b334ffc4b81",
   "metadata": {},
   "source": [
    "## Prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ff4e9c9-1907-4ef7-8b3c-8d1972abf180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading diabetes data...\n",
      "\n",
      "Demographic Distribution Analysis:\n",
      "\n",
      "Race distribution:\n",
      "race\n",
      "Caucasian          76099\n",
      "AfricanAmerican    19210\n",
      "?                   2273\n",
      "Hispanic            2037\n",
      "Other               1506\n",
      "Asian                641\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Gender distribution:\n",
      "gender\n",
      "Female             54708\n",
      "Male               47055\n",
      "Unknown/Invalid        3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Intersectional demographics (Race x Gender):\n",
      "gender           Female   Male  Unknown/Invalid\n",
      "race                                           \n",
      "?                  1133   1138                2\n",
      "AfricanAmerican   11728   7482                0\n",
      "Asian               318    323                0\n",
      "Caucasian         39689  36410                0\n",
      "Hispanic           1092    945                0\n",
      "Other               748    757                1\n",
      "\n",
      "Readmission Rate Analysis:\n",
      "\n",
      "Readmission rates by race:\n",
      "                 readmission_rate  sample_size\n",
      "race                                          \n",
      "?                        0.319402         2273\n",
      "AfricanAmerican          0.457522        19210\n",
      "Asian                    0.352574          641\n",
      "Caucasian                0.469336        76099\n",
      "Hispanic                 0.419244         2037\n",
      "Other                    0.392430         1506\n",
      "\n",
      "Readmission rates by gender:\n",
      "                 readmission_rate  sample_size\n",
      "gender                                        \n",
      "Female                   0.469218        54708\n",
      "Male                     0.451217        47055\n",
      "Unknown/Invalid          0.000000            3\n",
      "\n",
      "Intersectional readmission rates:\n",
      "                                 readmission_rate  sample_size\n",
      "race            gender                                        \n",
      "?               Female                   0.310680         1133\n",
      "                Male                     0.328647         1138\n",
      "                Unknown/Invalid          0.000000            2\n",
      "AfricanAmerican Female                   0.460607        11728\n",
      "                Male                     0.452686         7482\n",
      "Asian           Female                   0.355346          318\n",
      "                Male                     0.349845          323\n",
      "Caucasian       Female                   0.479931        39689\n",
      "                Male                     0.457786        36410\n",
      "Hispanic        Female                   0.415751         1092\n",
      "                Male                     0.423280          945\n",
      "Other           Female                   0.402406          748\n",
      "                Male                     0.383091          757\n",
      "                Unknown/Invalid          0.000000            1\n",
      "\n",
      "Treatment Pattern Analysis:\n",
      "\n",
      "metformin usage rates:\n",
      "\n",
      "By race:\n",
      "race\n",
      "?                  0.0\n",
      "AfricanAmerican    0.0\n",
      "Asian              0.0\n",
      "Caucasian          0.0\n",
      "Hispanic           0.0\n",
      "Other              0.0\n",
      "Name: metformin, dtype: float64\n",
      "\n",
      "By gender:\n",
      "gender\n",
      "Female             0.0\n",
      "Male               0.0\n",
      "Unknown/Invalid    0.0\n",
      "Name: metformin, dtype: float64\n",
      "\n",
      "insulin usage rates:\n",
      "\n",
      "By race:\n",
      "race\n",
      "?                  0.0\n",
      "AfricanAmerican    0.0\n",
      "Asian              0.0\n",
      "Caucasian          0.0\n",
      "Hispanic           0.0\n",
      "Other              0.0\n",
      "Name: insulin, dtype: float64\n",
      "\n",
      "By gender:\n",
      "gender\n",
      "Female             0.0\n",
      "Male               0.0\n",
      "Unknown/Invalid    0.0\n",
      "Name: insulin, dtype: float64\n",
      "\n",
      "diabetesMed usage rates:\n",
      "\n",
      "By race:\n",
      "race\n",
      "?                  0.823141\n",
      "AfricanAmerican    0.770328\n",
      "Asian              0.741030\n",
      "Caucasian          0.768078\n",
      "Hispanic           0.761414\n",
      "Other              0.808765\n",
      "Name: diabetesMed, dtype: float64\n",
      "\n",
      "By gender:\n",
      "gender\n",
      "Female             0.763819\n",
      "Male               0.777261\n",
      "Unknown/Invalid    0.666667\n",
      "Name: diabetesMed, dtype: float64\n",
      "\n",
      "Disparity Metrics:\n",
      "\n",
      "Race disparity ratio (max/min readmission rate): 1.47\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 119\u001b[0m\n\u001b[1;32m    117\u001b[0m analyze_treatment_disparities(df)\n\u001b[1;32m    118\u001b[0m plot_demographic_disparities(race_dist, gender_dist, race_readmission, gender_readmission)\n\u001b[0;32m--> 119\u001b[0m calculate_disparity_metrics(race_readmission, gender_readmission)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Print recommendations\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRecommendations for Fair Model Training:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 110\u001b[0m, in \u001b[0;36mcalculate_disparity_metrics\u001b[0;34m(race_readmission, gender_readmission)\u001b[0m\n\u001b[1;32m    108\u001b[0m gender_max_rate \u001b[38;5;241m=\u001b[39m gender_readmission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadmission_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m    109\u001b[0m gender_min_rate \u001b[38;5;241m=\u001b[39m gender_readmission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadmission_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\n\u001b[0;32m--> 110\u001b[0m gender_disparity_ratio \u001b[38;5;241m=\u001b[39m gender_max_rate \u001b[38;5;241m/\u001b[39m gender_min_rate\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGender disparity ratio (max/min readmission rate): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgender_disparity_ratio\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "#LLM code:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading diabetes data...\")\n",
    "df = pd.read_csv('../diabetesdatasets/diabetic_data.csv')\n",
    "\n",
    "def analyze_demographic_distribution(df):\n",
    "    # Analyze race and gender distribution\n",
    "    print(\"\\nDemographic Distribution Analysis:\")\n",
    "    print(\"\\nRace distribution:\")\n",
    "    race_dist = df['race'].value_counts()\n",
    "    print(race_dist)\n",
    "    print(\"\\nGender distribution:\")\n",
    "    gender_dist = df['gender'].value_counts()\n",
    "    print(gender_dist)\n",
    "    \n",
    "    # Calculate intersectional demographics\n",
    "    print(\"\\nIntersectional demographics (Race x Gender):\")\n",
    "    intersectional = pd.crosstab(df['race'], df['gender'])\n",
    "    print(intersectional)\n",
    "    \n",
    "    return race_dist, gender_dist, intersectional\n",
    "\n",
    "def analyze_outcome_disparities(df):\n",
    "    # Analyze readmission rates by demographic groups\n",
    "    print(\"\\nReadmission Rate Analysis:\")\n",
    "    \n",
    "    # Convert readmission to binary (any readmission vs no readmission)\n",
    "    df['readmitted_binary'] = df['readmitted'].map({'NO': 0, '>30': 1, '<30': 1})\n",
    "    \n",
    "    # Calculate readmission rates by race\n",
    "    race_readmission = df.groupby('race')['readmitted_binary'].agg(['mean', 'count'])\n",
    "    race_readmission.columns = ['readmission_rate', 'sample_size']\n",
    "    print(\"\\nReadmission rates by race:\")\n",
    "    print(race_readmission)\n",
    "    \n",
    "    # Calculate readmission rates by gender\n",
    "    gender_readmission = df.groupby('gender')['readmitted_binary'].agg(['mean', 'count'])\n",
    "    gender_readmission.columns = ['readmission_rate', 'sample_size']\n",
    "    print(\"\\nReadmission rates by gender:\")\n",
    "    print(gender_readmission)\n",
    "    \n",
    "    # Calculate intersectional readmission rates\n",
    "    intersectional_readmission = df.groupby(['race', 'gender'])['readmitted_binary'].agg(['mean', 'count'])\n",
    "    intersectional_readmission.columns = ['readmission_rate', 'sample_size']\n",
    "    print(\"\\nIntersectional readmission rates:\")\n",
    "    print(intersectional_readmission)\n",
    "    \n",
    "    return race_readmission, gender_readmission, intersectional_readmission\n",
    "\n",
    "def analyze_treatment_disparities(df):\n",
    "    # Analyze treatment patterns across demographics\n",
    "    treatment_cols = ['metformin', 'insulin', 'diabetesMed']\n",
    "    \n",
    "    print(\"\\nTreatment Pattern Analysis:\")\n",
    "    for treatment in treatment_cols:\n",
    "        print(f\"\\n{treatment} usage rates:\")\n",
    "        # By race\n",
    "        race_treatment = df.groupby('race')[treatment].apply(lambda x: (x == 'Yes').mean())\n",
    "        print(\"\\nBy race:\")\n",
    "        print(race_treatment)\n",
    "        # By gender\n",
    "        gender_treatment = df.groupby('gender')[treatment].apply(lambda x: (x == 'Yes').mean())\n",
    "        print(\"\\nBy gender:\")\n",
    "        print(gender_treatment)\n",
    "\n",
    "def plot_demographic_disparities(race_dist, gender_dist, race_readmission, gender_readmission):\n",
    "    # Create a figure with multiple subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot demographic distributions\n",
    "    race_dist.plot(kind='bar', ax=ax1, title='Race Distribution')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    gender_dist.plot(kind='bar', ax=ax2, title='Gender Distribution')\n",
    "    ax2.set_ylabel('Count')\n",
    "    \n",
    "    # Plot readmission rates\n",
    "    race_readmission['readmission_rate'].plot(kind='bar', ax=ax3, title='Readmission Rates by Race')\n",
    "    ax3.set_ylabel('Readmission Rate')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    gender_readmission['readmission_rate'].plot(kind='bar', ax=ax4, title='Readmission Rates by Gender')\n",
    "    ax4.set_ylabel('Readmission Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('diabetes_disparities.png')\n",
    "    plt.close()\n",
    "\n",
    "def calculate_disparity_metrics(race_readmission, gender_readmission):\n",
    "    # Calculate disparity metrics\n",
    "    print(\"\\nDisparity Metrics:\")\n",
    "    \n",
    "    # Race disparities\n",
    "    race_max_rate = race_readmission['readmission_rate'].max()\n",
    "    race_min_rate = race_readmission['readmission_rate'].min()\n",
    "    race_disparity_ratio = race_max_rate / race_min_rate\n",
    "    \n",
    "    print(f\"\\nRace disparity ratio (max/min readmission rate): {race_disparity_ratio:.2f}\")\n",
    "    \n",
    "    # Gender disparities\n",
    "    gender_max_rate = gender_readmission['readmission_rate'].max()\n",
    "    gender_min_rate = gender_readmission['readmission_rate'].min()\n",
    "    gender_disparity_ratio = gender_max_rate / gender_min_rate\n",
    "    \n",
    "    print(f\"Gender disparity ratio (max/min readmission rate): {gender_disparity_ratio:.2f}\")\n",
    "\n",
    "# Main execution\n",
    "race_dist, gender_dist, intersectional = analyze_demographic_distribution(df)\n",
    "race_readmission, gender_readmission, intersectional_readmission = analyze_outcome_disparities(df)\n",
    "analyze_treatment_disparities(df)\n",
    "plot_demographic_disparities(race_dist, gender_dist, race_readmission, gender_readmission)\n",
    "calculate_disparity_metrics(race_readmission, gender_readmission)\n",
    "\n",
    "# Print recommendations\n",
    "print(\"\\nRecommendations for Fair Model Training:\")\n",
    "print(\"\\n1. Data Rebalancing Strategies:\")\n",
    "for race, count in race_dist.items():\n",
    "    if count / len(df) < 0.1:  # If race represents less than 10% of the data\n",
    "        print(f\"- Consider oversampling {race} patients to increase representation\")\n",
    "\n",
    "print(\"\\n2. Model Development Recommendations:\")\n",
    "print(\"- Use stratified sampling for train/test splits based on both race and gender\")\n",
    "print(\"- Implement weighted loss functions to account for demographic imbalances\")\n",
    "print(\"- Monitor and validate performance separately for each demographic group\")\n",
    "\n",
    "print(\"\\n3. Clinical Implications:\")\n",
    "print(\"- Consider creating separate risk thresholds for different demographic groups\")\n",
    "print(\"- Implement regular bias audits in the deployment pipeline\")\n",
    "print(\"- Document any identified disparities for clinical review\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c29607-3ae6-4194-84b1-ba6d62068eda",
   "metadata": {},
   "source": [
    "### Output/Note\n",
    "Float division by zero for \n",
    "To fix this:\n",
    "Drop any groups with zero readmission rate before computing the ratio,\n",
    "Exclude very small groups (e.g. fewer than a minimum sample size), or\n",
    "Compute disparities using a difference (max – min) instead of a ratio when the min can be zero.\n",
    "\n",
    "The dataset is predominantly composed of Caucasian patients (76,099) and females (54,708), with smaller representations of Black (19,210), Hispanic (2,037), Asian (641), and \"unknown/other\" groups. Readmission rates by race range from 31.9 percent in the unknown-race group to 47.0 percent among Caucasians, indicating a disparity of approximately 1.47 times. Females have a readmission rate of 46.9 percent compared to 45.1 percent for males. Intersectional patterns by race and gender reflect similar trends.\n",
    "\n",
    "Anomalies are present in the data: both metformin and insulin show 0 percent usage, which is likely due to data coding errors. However, the variable \"diabetesMed\" indicates prescription rates ranging from approximately 74 percent among Asian patients to 82 percent among those in the \"unknown\" category, with both male and female groups averaging between 76 and 78 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d1fcf91-b118-440e-9ed7-db1ab0647867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM code 1\n",
    "# Implement weighted sampling strategy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_balanced_weights(df):\n",
    "    weights = np.ones(len(df))\n",
    "    \n",
    "    # Race weights (inverse frequency)\n",
    "    race_weights = 1 / df['race'].value_counts(normalize=True)\n",
    "    for race in race_weights.index:\n",
    "        weights[df['race'] == race] *= race_weights[race]\n",
    "    \n",
    "    # Adjust for outcome imbalance within groups\n",
    "    for race in df['race'].unique():\n",
    "        race_mask = df['race'] == race\n",
    "        readmit_rate = df[race_mask]['readmitted_binary'].mean()\n",
    "        weights[race_mask & (df['readmitted_binary'] == 1)] *= (1 / readmit_rate)\n",
    "        weights[race_mask & (df['readmitted_binary'] == 0)] *= (1 / (1 - readmit_rate))\n",
    "    \n",
    "    return weights\n",
    "\n",
    "df['readmitted_binary'] = (df['readmitted'] == '<30').astype(int)\n",
    "sample_weights = calculate_balanced_weights(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fce57a6-e317-479d-aa27-b7f444a69c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "?                   7.461945\n",
      "AfricanAmerican     0.882926\n",
      "Asian              26.460218\n",
      "Caucasian           0.222881\n",
      "Hispanic            8.326460\n",
      "Other              11.262284\n",
      "Name: sample_weight, dtype: float64\n",
      "race\n",
      "?                  0.5\n",
      "AfricanAmerican    0.5\n",
      "Asian              0.5\n",
      "Caucasian          0.5\n",
      "Hispanic           0.5\n",
      "Other              0.5\n",
      "Name: sample_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#New code 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load and prepare\n",
    "df = pd.read_csv(\"../diabetesdatasets/diabetic_data.csv\")\n",
    "df[\"readmitted_binary\"] = df[\"readmitted\"].map({\"NO\":0, \">30\":1, \"<30\":1})\n",
    "\n",
    "# 2. Compute weights\n",
    "def calculate_balanced_weights(df):\n",
    "    # start with all ones\n",
    "    weights = np.ones(len(df), dtype=float)\n",
    "    \n",
    "    # a) inverse-frequency by race\n",
    "    race_freq = df[\"race\"].value_counts(normalize=True)\n",
    "    for race, freq in race_freq.items():\n",
    "        weights[df[\"race\"] == race] *= 1.0 / freq\n",
    "\n",
    "    # b) within-race outcome balancing\n",
    "    for race in df[\"race\"].unique():\n",
    "        mask = df[\"race\"] == race\n",
    "        # skip races with zero positives or negatives\n",
    "        pos_rate = df.loc[mask, \"readmitted_binary\"].mean()\n",
    "        if pos_rate in (0,1):\n",
    "            continue\n",
    "        # boost positives and negatives inversely to their within-race rates\n",
    "        weights[mask & (df[\"readmitted_binary\"] == 1)] *= 1.0 / pos_rate\n",
    "        weights[mask & (df[\"readmitted_binary\"] == 0)] *= 1.0 / (1.0 - pos_rate)\n",
    "    \n",
    "    # normalize so average weight = 1\n",
    "    return weights / np.mean(weights)\n",
    "\n",
    "df[\"sample_weight\"] = calculate_balanced_weights(df)\n",
    "\n",
    "# 3. Quick check\n",
    "print(df.groupby(\"race\")[\"sample_weight\"].mean())\n",
    "print(df.groupby(\"race\")[\"sample_weight\"].apply(lambda x: np.sum(x[df.loc[x.index, \"readmitted_binary\"]==1]) / np.sum(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b13af139-463d-4cab-978e-8b2048127dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM code 2:\n",
    "#sampling strategies\n",
    "# Two-phase sampling strategy\n",
    "def balanced_sample_generator(df):\n",
    "    # Phase 1: Oversample minority races\n",
    "    race_sampler = SMOTE(sampling_strategy={\n",
    "        'Asian': 15000,  # Increase to ~15% representation\n",
    "        'Hispanic': 15000,\n",
    "        'Other': 15000\n",
    "    })\n",
    "    \n",
    "    # Phase 2: Balance outcomes within each racial group\n",
    "    stratified_sampler = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    return combined_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fbef945-1645-459f-8803-80dc6de81cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversample: Counter({'Caucasian': 76099, 'AfricanAmerican': 19210, '?': 2273, 'Hispanic': 2037, 'Other': 1506, 'Asian': 641})\n",
      "After  oversample: Counter({'Caucasian': 76099, 'AfricanAmerican': 19210, 'Other': 15264, 'Asian': 15264, 'Hispanic': 15264, '?': 2273})\n",
      "Fold 0: train=114699, val=28675\n",
      "Fold 1: train=114699, val=28675\n",
      "Fold 2: train=114699, val=28675\n",
      "Fold 3: train=114699, val=28675\n",
      "Fold 4: train=114700, val=28674\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 1. Load and prepare\n",
    "df = pd.read_csv(\"../diabetesdatasets/diabetic_data.csv\")\n",
    "df[\"readmitted_binary\"] = df[\"readmitted\"].map({\"NO\":0, \">30\":1, \"<30\":1})\n",
    "\n",
    "# 2. Phase 1: Randomly oversample by race\n",
    "#    target ~15% of total for each minority race\n",
    "total = len(df)\n",
    "target_n = int(0.15 * total)\n",
    "\n",
    "ros = RandomOverSampler(\n",
    "    sampling_strategy={\n",
    "        \"Asian\":    target_n,\n",
    "        \"Hispanic\": target_n,\n",
    "        \"Other\":    target_n\n",
    "    },\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit_resample on the entire DataFrame and race column\n",
    "df_ros, race_ros = ros.fit_resample(df, df[\"race\"])\n",
    "print(\"Before oversample:\", Counter(df[\"race\"]))\n",
    "print(\"After  oversample:\", Counter(race_ros))\n",
    "\n",
    "# 3. Prepare X, y, and grouping column\n",
    "X_ros = df_ros.drop(columns=[\"race\", \"readmitted\", \"readmitted_binary\"])\n",
    "y_ros = df_ros[\"readmitted_binary\"].values\n",
    "groups = df_ros[\"race\"].astype(str) + \"__\" + df_ros[\"gender\"].astype(str)\n",
    "\n",
    "# 4. Phase 2: Stratified folds by race+gender\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_ros, y_ros, groups)):\n",
    "    print(f\"Fold {fold}: train={len(train_idx)}, val={len(val_idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "993b80ff-9eb3-408c-b5b1-1f9e94a2264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM code 3\n",
    "#model level adjustments\n",
    "# Custom loss function with fairness constraints\n",
    "def fairness_aware_loss(y_true, y_pred, group_membership):\n",
    "    base_loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Add demographic parity constraint\n",
    "    demographic_penalty = calculate_demographic_disparity(y_pred, group_membership)\n",
    "    \n",
    "    # Add equal opportunity constraint\n",
    "    opportunity_penalty = calculate_equal_opportunity_disparity(y_true, y_pred, group_membership)\n",
    "    \n",
    "    return base_loss + 0.1 * demographic_penalty + 0.1 * opportunity_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7cbe2a2f-56e0-40bc-8caf-75f6aaa989e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 11:46:18.057210: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 11:46:19.341149: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 11:46:20.618375: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 11:46:21.937839: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 average loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 11:46:23.157321: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 average loss: nan\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722us/step\n",
      "Test fairness-aware loss: nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute         import SimpleImputer\n",
    "from sklearn.preprocessing  import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose        import ColumnTransformer\n",
    "from sklearn.pipeline       import Pipeline\n",
    "\n",
    "# 1. Load data and create binary readmission label\n",
    "df = pd.read_csv(\"../diabetesdatasets/diabetic_data.csv\")\n",
    "df[\"readmitted_binary\"] = df[\"readmitted\"].map({\"NO\": 0, \">30\": 1, \"<30\": 1})\n",
    "\n",
    "# 2. Separate features and target\n",
    "y = df[\"readmitted_binary\"]\n",
    "X = df.drop(columns=[\"readmitted\", \"readmitted_binary\"])\n",
    "\n",
    "# 3. Train–test split stratified on race\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=df[\"race\"], random_state=42\n",
    ")\n",
    "\n",
    "# 4. Auto-detect categorical vs numeric\n",
    "cat_cols = X_train_df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = [c for c in X_train_df.columns if c not in cat_cols]\n",
    "\n",
    "# 5. Build preprocessing pipelines\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\",  OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scale\",   StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", cat_pipeline, cat_cols),\n",
    "    (\"num\", num_pipeline, num_cols)\n",
    "])\n",
    "\n",
    "# 6. Fit and transform\n",
    "X_train_proc = preprocessor.fit_transform(X_train_df)\n",
    "X_test_proc  = preprocessor.transform(X_test_df)\n",
    "\n",
    "# 7. Convert to dense numpy arrays\n",
    "if not isinstance(X_train_proc, np.ndarray):\n",
    "    X_train_np = X_train_proc.toarray()\n",
    "else:\n",
    "    X_train_np = X_train_proc\n",
    "if not isinstance(X_test_proc, np.ndarray):\n",
    "    X_test_np = X_test_proc.toarray()\n",
    "else:\n",
    "    X_test_np = X_test_proc\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_test_np  = y_test.to_numpy()\n",
    "\n",
    "# 8. Build race_group marker (1 for Black, 0 otherwise)\n",
    "race_group = (X_train_df[\"race\"] == \"Black\").astype(int).to_numpy()\n",
    "race_group_test = (X_test_df[\"race\"] == \"Black\").astype(int).to_numpy()\n",
    "\n",
    "# 9. Define Keras model\n",
    "inputs = tf.keras.Input(shape=(X_train_np.shape[1],))\n",
    "x      = tf.keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model  = tf.keras.Model(inputs, output)\n",
    "\n",
    "# 10. Fairness-aware loss with proper casting\n",
    "def fairness_aware_loss(y_true, y_pred, group):\n",
    "    # y_true: (batch,), cast to float32\n",
    "    y_true_f = tf.cast(y_true, tf.float32)\n",
    "    # y_pred: (batch,1), squeeze to (batch,)\n",
    "    y_pred_s = tf.squeeze(y_pred, axis=-1)\n",
    "    \n",
    "    # base BCE\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true_f, y_pred_s)\n",
    "    \n",
    "    # demographic parity penalty\n",
    "    g0 = tf.boolean_mask(y_pred_s, tf.equal(group, 0))\n",
    "    g1 = tf.boolean_mask(y_pred_s, tf.equal(group, 1))\n",
    "    dp = tf.abs(tf.reduce_mean(g0) - tf.reduce_mean(g1))\n",
    "    \n",
    "    # equal opportunity penalty\n",
    "    y0_true = tf.boolean_mask(y_true_f, tf.equal(group, 0))\n",
    "    y1_true = tf.boolean_mask(y_true_f, tf.equal(group, 1))\n",
    "    y0_pred = tf.boolean_mask(y_pred_s, tf.equal(group, 0))\n",
    "    y1_pred = tf.boolean_mask(y_pred_s, tf.equal(group, 1))\n",
    "    \n",
    "    tpr0 = tf.reduce_sum(tf.cast(y0_pred > 0.5, tf.float32) * y0_true) / (tf.reduce_sum(y0_true) + 1e-6)\n",
    "    tpr1 = tf.reduce_sum(tf.cast(y1_pred > 0.5, tf.float32) * y1_true) / (tf.reduce_sum(y1_true) + 1e-6)\n",
    "    eo   = tf.abs(tpr0 - tpr1)\n",
    "    \n",
    "    return bce + 0.1 * dp + 0.1 * eo\n",
    "\n",
    "# 11. Build tf.data.Dataset\n",
    "batch_size = 128\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train_np, y_train_np, race_group))\n",
    "dataset = dataset.shuffle(5000, seed=42).batch(batch_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "@tf.function\n",
    "def train_step(xb, yb, gb):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(xb, training=True)\n",
    "        loss  = fairness_aware_loss(yb, preds, gb)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# 12. Training loop\n",
    "for epoch in range(5):\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    for xb, yb, gb in dataset:\n",
    "        total_loss += train_step(xb, yb, gb)\n",
    "        steps += 1\n",
    "    print(f\"Epoch {epoch+1} average loss: {total_loss/steps:.4f}\")\n",
    "\n",
    "# 13. Test evaluation\n",
    "y_proba_test = model.predict(X_test_np)\n",
    "test_loss    = tf.reduce_mean(fairness_aware_loss(y_test_np, y_proba_test, race_group_test))\n",
    "print(\"Test fairness-aware loss:\", test_loss.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16253723-0dd3-444b-81fb-9344cc2148bc",
   "metadata": {},
   "source": [
    "The training procedure produced “nan” for the average loss in every epoch and likewise returned a non-number for the test fairness-aware loss. At the same time, TensorFlow emitted “OUT_OF_RANGE: End of sequence” warnings as it exhausted the dataset iterator. Taken together, these results indicate that the custom loss function became numerically unstable most likely because one of the demographic groups had no positive or negative examples in a batch, causing divisions by zero or empty tensors in the demographic parity or equal opportunity terms. As a result, the model was unable to learn meaningful gradients and all loss values collapsed to “nan.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17449ae9-fdae-40f9-969d-5171f6312f60",
   "metadata": {},
   "source": [
    "## Prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4db3aba8-c9d3-42b2-b9fb-4fd05ab80530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading diabetes data...\n",
      "\n",
      "Calculating fairness metrics by race...\n",
      "\n",
      "Calculating fairness metrics by gender...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_21003/455804636.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group_data['risk_score'] = group_data[risk_factors].mean(axis=1)\n",
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_21003/455804636.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group_data['risk_score'] = group_data[risk_factors].mean(axis=1)\n",
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_21003/455804636.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group_data['risk_score'] = group_data[risk_factors].mean(axis=1)\n",
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_21003/455804636.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group_data['risk_score'] = group_data[risk_factors].mean(axis=1)\n",
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_21003/455804636.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group_data['risk_score'] = group_data[risk_factors].mean(axis=1)\n",
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_21003/455804636.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group_data['risk_score'] = group_data[risk_factors].mean(axis=1)\n",
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_21003/455804636.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group_data['risk_score'] = group_data[risk_factors].mean(axis=1)\n",
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_21003/455804636.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group_data['risk_score'] = group_data[risk_factors].mean(axis=1)\n",
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_21003/455804636.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group_data['risk_score'] = group_data[risk_factors].mean(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing treatment equity...\n",
      "\n",
      "Fairness Metrics by Race:\n",
      "\n",
      "Caucasian:\n",
      "  group_size: 76099.000\n",
      "  representation: 0.748\n",
      "  readmission_rate: 0.469\n",
      "  statistical_parity_diff: 0.008\n",
      "  disparate_impact: 1.018\n",
      "  false_positive_rate: 0.461\n",
      "  false_negative_rate: 0.486\n",
      "  true_positive_rate: 0.514\n",
      "  positive_predictive_value: 0.497\n",
      "\n",
      "AfricanAmerican:\n",
      "  group_size: 19210.000\n",
      "  representation: 0.189\n",
      "  readmission_rate: 0.458\n",
      "  statistical_parity_diff: -0.003\n",
      "  disparate_impact: 0.993\n",
      "  false_positive_rate: 0.459\n",
      "  false_negative_rate: 0.490\n",
      "  true_positive_rate: 0.510\n",
      "  positive_predictive_value: 0.484\n",
      "\n",
      "?:\n",
      "  group_size: 2273.000\n",
      "  representation: 0.022\n",
      "  readmission_rate: 0.319\n",
      "  statistical_parity_diff: -0.141\n",
      "  disparate_impact: 0.693\n",
      "  false_positive_rate: 0.460\n",
      "  false_negative_rate: 0.435\n",
      "  true_positive_rate: 0.565\n",
      "  positive_predictive_value: 0.366\n",
      "\n",
      "Other:\n",
      "  group_size: 1506.000\n",
      "  representation: 0.015\n",
      "  readmission_rate: 0.392\n",
      "  statistical_parity_diff: -0.068\n",
      "  disparate_impact: 0.851\n",
      "  false_positive_rate: 0.458\n",
      "  false_negative_rate: 0.440\n",
      "  true_positive_rate: 0.560\n",
      "  positive_predictive_value: 0.441\n",
      "\n",
      "Asian:\n",
      "  group_size: 641.000\n",
      "  representation: 0.006\n",
      "  readmission_rate: 0.353\n",
      "  statistical_parity_diff: -0.108\n",
      "  disparate_impact: 0.765\n",
      "  false_positive_rate: 0.484\n",
      "  false_negative_rate: 0.473\n",
      "  true_positive_rate: 0.527\n",
      "  positive_predictive_value: 0.372\n",
      "\n",
      "Hispanic:\n",
      "  group_size: 2037.000\n",
      "  representation: 0.020\n",
      "  readmission_rate: 0.419\n",
      "  statistical_parity_diff: -0.042\n",
      "  disparate_impact: 0.910\n",
      "  false_positive_rate: 0.467\n",
      "  false_negative_rate: 0.456\n",
      "  true_positive_rate: 0.544\n",
      "  positive_predictive_value: 0.457\n",
      "\n",
      "Fairness Metrics by Gender:\n",
      "\n",
      "Female:\n",
      "  group_size: 54708.000\n",
      "  representation: 0.538\n",
      "  readmission_rate: 0.469\n",
      "  statistical_parity_diff: 0.008\n",
      "  disparate_impact: 1.018\n",
      "  false_positive_rate: 0.466\n",
      "  false_negative_rate: 0.472\n",
      "  true_positive_rate: 0.528\n",
      "  positive_predictive_value: 0.500\n",
      "\n",
      "Male:\n",
      "  group_size: 47055.000\n",
      "  representation: 0.462\n",
      "  readmission_rate: 0.451\n",
      "  statistical_parity_diff: -0.010\n",
      "  disparate_impact: 0.979\n",
      "  false_positive_rate: 0.466\n",
      "  false_negative_rate: 0.487\n",
      "  true_positive_rate: 0.513\n",
      "  positive_predictive_value: 0.475\n",
      "\n",
      "Unknown/Invalid:\n",
      "  group_size: 3.000\n",
      "  representation: 0.000\n",
      "  readmission_rate: 0.000\n",
      "  statistical_parity_diff: -0.461\n",
      "  disparate_impact: 0.000\n",
      "  false_positive_rate: 0.333\n",
      "  false_negative_rate: 0.000\n",
      "  true_positive_rate: 0.000\n",
      "  positive_predictive_value: 0.000\n",
      "\n",
      "Treatment Equity Analysis:\n",
      "\n",
      "metformin rates by race:\n",
      "race\n",
      "?                  0.0\n",
      "AfricanAmerican    0.0\n",
      "Asian              0.0\n",
      "Caucasian          0.0\n",
      "Hispanic           0.0\n",
      "Other              0.0\n",
      "Name: metformin, dtype: float64\n",
      "\n",
      "insulin rates by race:\n",
      "race\n",
      "?                  0.0\n",
      "AfricanAmerican    0.0\n",
      "Asian              0.0\n",
      "Caucasian          0.0\n",
      "Hispanic           0.0\n",
      "Other              0.0\n",
      "Name: insulin, dtype: float64\n",
      "\n",
      "diabetesMed rates by race:\n",
      "race\n",
      "?                  0.823141\n",
      "AfricanAmerican    0.770328\n",
      "Asian              0.741030\n",
      "Caucasian          0.768078\n",
      "Hispanic           0.761414\n",
      "Other              0.808765\n",
      "Name: diabetesMed, dtype: float64\n",
      "\n",
      "Fairness Concerns Assessment:\n",
      "\n",
      "Race Disparity Analysis:\n",
      "- Concerning disparity in readmission rates: 0.150\n",
      "\n",
      "Gender Disparity Analysis:\n",
      "- Concerning disparity in readmission rates: 0.469\n",
      "- Concerning disparity in false positive rates: 0.133\n",
      "- Concerning disparity in false negative rates: 0.487\n",
      "\n",
      "Recommendations:\n",
      "1. Data Collection and Representation:\n",
      "- Increase data collection for ? patients\n",
      "- Increase data collection for Other patients\n",
      "- Increase data collection for Asian patients\n",
      "- Increase data collection for Hispanic patients\n",
      "\n",
      "2. Model Development:\n",
      "- Implement fairness constraints in the model\n",
      "- Use stratified sampling for train/test splits\n",
      "- Consider separate models or calibration for different demographic groups\n",
      "\n",
      "3. Clinical Practice:\n",
      "- Review treatment decision protocols for potential bias\n",
      "- Implement regular fairness audits\n",
      "- Document and monitor disparities over time\n"
     ]
    }
   ],
   "source": [
    "#llm code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading diabetes data...\")\n",
    "df = pd.read_csv('../diabetesdatasets/diabetic_data.csv')\n",
    "\n",
    "def calculate_fairness_metrics(df, group_col, outcome_col='readmitted'):\n",
    "    \"\"\"Calculate various fairness metrics for each group.\"\"\"\n",
    "    \n",
    "    # Convert readmission to binary (any readmission vs no readmission)\n",
    "    df['outcome_binary'] = (df[outcome_col] != 'NO').astype(int)\n",
    "    \n",
    "    metrics = {}\n",
    "    groups = df[group_col].unique()\n",
    "    \n",
    "    # Calculate overall readmission rate (base rate)\n",
    "    overall_rate = df['outcome_binary'].mean()\n",
    "    \n",
    "    for group in groups:\n",
    "        group_data = df[df[group_col] == group]\n",
    "        if len(group_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Basic statistics\n",
    "        group_size = len(group_data)\n",
    "        group_rate = group_data['outcome_binary'].mean()\n",
    "        \n",
    "        # Calculate disparity metrics\n",
    "        statistical_parity_diff = group_rate - overall_rate\n",
    "        disparate_impact = group_rate / overall_rate if overall_rate > 0 else 0\n",
    "        \n",
    "        # Calculate confusion matrix based metrics\n",
    "        # For this analysis, we'll use the actual readmission as ground truth\n",
    "        # and compare with a \"predicted\" readmission based on risk factors\n",
    "        \n",
    "        # Create a simple risk score based on available features\n",
    "        risk_factors = ['time_in_hospital', 'num_lab_procedures', 'num_medications']\n",
    "        group_data['risk_score'] = group_data[risk_factors].mean(axis=1)\n",
    "        predicted_high_risk = (group_data['risk_score'] > group_data['risk_score'].median()).astype(int)\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(\n",
    "            group_data['outcome_binary'],\n",
    "            predicted_high_risk\n",
    "        ).ravel()\n",
    "        \n",
    "        # Calculate fairness metrics\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        \n",
    "        metrics[group] = {\n",
    "            'group_size': group_size,\n",
    "            'representation': group_size / len(df),\n",
    "            'readmission_rate': group_rate,\n",
    "            'statistical_parity_diff': statistical_parity_diff,\n",
    "            'disparate_impact': disparate_impact,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'true_positive_rate': tpr,\n",
    "            'positive_predictive_value': ppv\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def analyze_treatment_equity(df, group_col):\n",
    "    \"\"\"Analyze treatment patterns across groups.\"\"\"\n",
    "    treatment_cols = ['metformin', 'insulin', 'diabetesMed']\n",
    "    treatment_metrics = {}\n",
    "    \n",
    "    for treatment in treatment_cols:\n",
    "        # Calculate treatment rates by group\n",
    "        treatment_rates = df.groupby(group_col)[treatment].apply(\n",
    "            lambda x: (x == 'Yes').mean()\n",
    "        )\n",
    "        treatment_metrics[treatment] = treatment_rates\n",
    "    \n",
    "    return treatment_metrics\n",
    "\n",
    "def plot_fairness_metrics(metrics, title):\n",
    "    \"\"\"Visualize key fairness metrics across groups.\"\"\"\n",
    "    groups = list(metrics.keys())\n",
    "    \n",
    "    # Select key metrics to plot\n",
    "    key_metrics = ['readmission_rate', 'false_positive_rate', 'false_negative_rate']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(key_metrics), figsize=(15, 5))\n",
    "    \n",
    "    for idx, metric in enumerate(key_metrics):\n",
    "        values = [metrics[group][metric] for group in groups]\n",
    "        axes[idx].bar(groups, values)\n",
    "        axes[idx].set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'diabetes_fairness_{title}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Calculate fairness metrics by race and gender\n",
    "print(\"\\nCalculating fairness metrics by race...\")\n",
    "race_metrics = calculate_fairness_metrics(df, 'race')\n",
    "print(\"\\nCalculating fairness metrics by gender...\")\n",
    "gender_metrics = calculate_fairness_metrics(df, 'gender')\n",
    "\n",
    "# Analyze treatment equity\n",
    "print(\"\\nAnalyzing treatment equity...\")\n",
    "race_treatment_metrics = analyze_treatment_equity(df, 'race')\n",
    "gender_treatment_metrics = analyze_treatment_equity(df, 'gender')\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nFairness Metrics by Race:\")\n",
    "for race, metrics in race_metrics.items():\n",
    "    print(f\"\\n{race}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nFairness Metrics by Gender:\")\n",
    "for gender, metrics in gender_metrics.items():\n",
    "    print(f\"\\n{gender}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nTreatment Equity Analysis:\")\n",
    "for treatment, rates in race_treatment_metrics.items():\n",
    "    print(f\"\\n{treatment} rates by race:\")\n",
    "    print(rates)\n",
    "\n",
    "# Generate visualizations\n",
    "plot_fairness_metrics(race_metrics, 'race')\n",
    "plot_fairness_metrics(gender_metrics, 'gender')\n",
    "\n",
    "# Evaluate fairness concerns\n",
    "print(\"\\nFairness Concerns Assessment:\")\n",
    "\n",
    "# Define thresholds for concerning disparities\n",
    "DISPARITY_THRESHOLD = 0.1  # 10% difference\n",
    "IMPACT_RATIO_THRESHOLD = 0.8  # 80% rule\n",
    "\n",
    "# Check for concerning disparities\n",
    "def assess_disparities(metrics, metric_name, threshold):\n",
    "    values = [m[metric_name] for m in metrics.values()]\n",
    "    max_val = max(values)\n",
    "    min_val = min(values)\n",
    "    disparity = max_val - min_val\n",
    "    return disparity > threshold, disparity\n",
    "\n",
    "# Assess various types of disparities\n",
    "for metrics_dict, group_type in [(race_metrics, 'Race'), (gender_metrics, 'Gender')]:\n",
    "    print(f\"\\n{group_type} Disparity Analysis:\")\n",
    "    \n",
    "    # Statistical Parity\n",
    "    concerning, disparity = assess_disparities(metrics_dict, 'readmission_rate', DISPARITY_THRESHOLD)\n",
    "    if concerning:\n",
    "        print(f\"- Concerning disparity in readmission rates: {disparity:.3f}\")\n",
    "    \n",
    "    # False Positive Rate\n",
    "    concerning, disparity = assess_disparities(metrics_dict, 'false_positive_rate', DISPARITY_THRESHOLD)\n",
    "    if concerning:\n",
    "        print(f\"- Concerning disparity in false positive rates: {disparity:.3f}\")\n",
    "    \n",
    "    # False Negative Rate\n",
    "    concerning, disparity = assess_disparities(metrics_dict, 'false_negative_rate', DISPARITY_THRESHOLD)\n",
    "    if concerning:\n",
    "        print(f\"- Concerning disparity in false negative rates: {disparity:.3f}\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"1. Data Collection and Representation:\")\n",
    "for group, metrics in race_metrics.items():\n",
    "    if metrics['representation'] < 0.1:  # Less than 10% representation\n",
    "        print(f\"- Increase data collection for {group} patients\")\n",
    "\n",
    "print(\"\\n2. Model Development:\")\n",
    "print(\"- Implement fairness constraints in the model\")\n",
    "print(\"- Use stratified sampling for train/test splits\")\n",
    "print(\"- Consider separate models or calibration for different demographic groups\")\n",
    "\n",
    "print(\"\\n3. Clinical Practice:\")\n",
    "print(\"- Review treatment decision protocols for potential bias\")\n",
    "print(\"- Implement regular fairness audits\")\n",
    "print(\"- Document and monitor disparities over time\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2330f-1697-42ad-a524-150806044df8",
   "metadata": {},
   "source": [
    "### output\n",
    "Calculated “fairness” metrics—group size, representation share, readmission rate, statistical‐parity difference, disparate‐impact ratio, false‐positive/negative rates, true‐positive rate and positive‐predictive value for each race and gender. A simple “risk score” (the mean of three numeric features) was thresholded at its median to produce predicted labels, which fed the confusion‐matrix–based rates. The results show that Caucasians and African Americans have readmission rates around 46–47% while smaller groups (e.g. “?” and “Other”) fall below, yielding a 15% readmission disparity by race and an even larger 47% gap by gender. Treatment equity checks revealed zero usage for metformin and insulin (likely a coding or data‐entry issue) but 74–82% use of “diabetesMed” across groups. Finally, the code flagged both race and gender readmission gaps as “concerning” and offered data-collection, modeling, and clinical recommendations. A recurring SettingWithCopyWarning arose when computing group‐level risk scores on slices of the DataFrame, indicating you should use .loc to avoid unintentionally modifying views rather than copies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26be295-0e0f-4f42-adaf-8d0700f9c7d0",
   "metadata": {},
   "source": [
    "# LLM provided code snippets\n",
    "\n",
    "-Training models with custom loss functions or sample weights.\n",
    "-Building fairness-aware cross-validation loops or SMOTE+CV pipelines.\n",
    "-Calibrating model outputs differently by group at inference.\n",
    "-Monitoring fairness metrics on live predictions or treatment decisions.\n",
    "\n",
    "They’re out of scope because my thesis focuses on identifying and correcting bias at the data and code‐analysis stages, not on building, training, or monitoring deployed predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "460519ae-24a1-4e33-8e8b-57057491446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm code 1\n",
    "#data collection and preprocessing\n",
    "def implement_balanced_sampling(df):\n",
    "    # Calculate sampling weights\n",
    "    weights = {\n",
    "        'Asian': 10.0,  # Increase Asian representation\n",
    "        'Hispanic': 5.0,  # Increase Hispanic representation\n",
    "        'Other': 5.0,    # Increase Other representation\n",
    "        'Caucasian': 1.0 # Reference group\n",
    "    }\n",
    "    \n",
    "    # Apply weights in sampling\n",
    "    df['sample_weight'] = df['race'].map(weights)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89439564-8b35-47c7-ab6b-62af26b032c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm code 2\n",
    "#model development\n",
    "def create_fair_model(X, y, sensitive_features):\n",
    "    # Implement fairness constraints\n",
    "    fairness_constraints = {\n",
    "        'statistical_parity_difference': 0.05,  # Max 5% difference\n",
    "        'equal_opportunity_difference': 0.05\n",
    "    }\n",
    "    \n",
    "    # Create balanced folds for cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    cv.split(X, y, sensitive_features)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03915dd4-2bf8-4691-b9b9-e223ed10a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm code 3\n",
    "#monitoring and validation\n",
    "def monitor_fairness_metrics(predictions, actual, sensitive_features):\n",
    "    metrics = {\n",
    "        'statistical_parity': calculate_statistical_parity(predictions, sensitive_features),\n",
    "        'equal_opportunity': calculate_equal_opportunity(predictions, actual, sensitive_features),\n",
    "        'disparate_impact': calculate_disparate_impact(predictions, sensitive_features)\n",
    "    }\n",
    "    \n",
    "    # Set alert thresholds\n",
    "    ALERT_THRESHOLDS = {\n",
    "        'statistical_parity': 0.05,\n",
    "        'equal_opportunity': 0.05,\n",
    "        'disparate_impact': 0.8\n",
    "    }\n",
    "    \n",
    "    return check_thresholds(metrics, ALERT_THRESHOLDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ae2029c-ff96-4edc-9047-f25b6c6fd80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm code 4\n",
    "#clinical implementation\n",
    "def calibrate_risk_scores(scores, race):\n",
    "    # Adjust thresholds by race to equalize false positive/negative rates\n",
    "    thresholds = {\n",
    "        'Asian': 0.45,  # Lower threshold due to underrepresentation\n",
    "        'Hispanic': 0.47,\n",
    "        'Caucasian': 0.50,\n",
    "        'AfricanAmerican': 0.50\n",
    "    }\n",
    "    return scores > thresholds[race]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03dbf83b-8bef-4096-a933-209aff87067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm code 5\n",
    "#treatment equity\n",
    "def monitor_treatment_equity(df):\n",
    "    # Calculate treatment rates by demographic group\n",
    "    treatment_rates = df.groupby(['race', 'gender'])['diabetesMed'].mean()\n",
    "    \n",
    "    # Check for significant disparities\n",
    "    disparity_threshold = 0.05  # 5% difference\n",
    "    return identify_treatment_disparities(treatment_rates, disparity_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88043fb3-ee16-4faa-9fe0-02e3ddf7dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New code for all 5 code blocks:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "\n",
    "# 1) Data collection and preprocessing\n",
    "def implement_balanced_sampling(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach a sample_weight column that up-weights underrepresented races.\n",
    "    \"\"\"\n",
    "    weights = {\n",
    "        'Asian':      10.0,\n",
    "        'Hispanic':    5.0,\n",
    "        'Other':       5.0,\n",
    "        'Caucasian':   1.0,\n",
    "        'AfricanAmerican': 2.0  # in case that category appears\n",
    "    }\n",
    "    # Default to 1.0 for any race not in the map\n",
    "    df['sample_weight'] = df['race'].map(weights).fillna(1.0)\n",
    "    return df\n",
    "\n",
    "# 2) Model development\n",
    "def create_fair_model(X: np.ndarray, y: np.ndarray, sensitive: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns a StratifiedKFold object and a placeholder model.\n",
    "    In practice you would wrap this CV with a fairness-aware learner.\n",
    "    \"\"\"\n",
    "    # Fairness constraints dictionary (not used directly here)\n",
    "    fairness_constraints = {\n",
    "        'statistical_parity_difference': 0.05,\n",
    "        'equal_opportunity_difference':  0.05\n",
    "    }\n",
    "\n",
    "    # Create balanced folds for cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    # Example of iterating through folds (no model training here)\n",
    "    for train_idx, val_idx in cv.split(X, y, sensitive):\n",
    "        # Normally train your model on X[train_idx], y[train_idx]\n",
    "        # and validate on X[val_idx], y[val_idx]\n",
    "        pass\n",
    "\n",
    "    # Return the CV instance and a dummy placeholder for the model\n",
    "    # Replace this with your actual fairness-aware estimator\n",
    "    return cv, None\n",
    "\n",
    "# 3) Monitoring and validation\n",
    "# Stub implementations for the metric calculators\n",
    "def calculate_statistical_parity(preds, sensitive):\n",
    "    # difference in positive rates between groups 0 and 1\n",
    "    rates = {}\n",
    "    for g in np.unique(sensitive):\n",
    "        rates[g] = np.mean(preds[sensitive == g])\n",
    "    return abs(rates.get(0, 0) - rates.get(1, 0))\n",
    "\n",
    "def calculate_equal_opportunity(preds, actual, sensitive):\n",
    "    # difference in true-positive rates\n",
    "    tprs = {}\n",
    "    for g in np.unique(sensitive):\n",
    "        mask = (sensitive == g)\n",
    "        true_pos = np.sum((preds[mask] == 1) & (actual[mask] == 1))\n",
    "        positives = np.sum(actual[mask] == 1)\n",
    "        tprs[g] = true_pos / positives if positives > 0 else 0\n",
    "    return abs(tprs.get(0, 0) - tprs.get(1, 0))\n",
    "\n",
    "def calculate_disparate_impact(preds, sensitive):\n",
    "    # ratio of positive rates\n",
    "    rates = {}\n",
    "    for g in np.unique(sensitive):\n",
    "        rates[g] = np.mean(preds[sensitive == g])\n",
    "    low, high = min(rates.values()), max(rates.values())\n",
    "    return low / high if high > 0 else np.nan\n",
    "\n",
    "def check_thresholds(metrics: dict, thresholds: dict) -> dict:\n",
    "    alerts = {}\n",
    "    for name, value in metrics.items():\n",
    "        alerts[name] = value > thresholds.get(name, np.inf)\n",
    "    return alerts\n",
    "\n",
    "def monitor_fairness_metrics(predictions: np.ndarray,\n",
    "                             actual:      np.ndarray,\n",
    "                             sensitive:  np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Compute common fairness metrics and check against alert thresholds.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'statistical_parity': calculate_statistical_parity(predictions, sensitive),\n",
    "        'equal_opportunity': calculate_equal_opportunity(predictions, actual, sensitive),\n",
    "        'disparate_impact':  calculate_disparate_impact(predictions, sensitive)\n",
    "    }\n",
    "    ALERT_THRESHOLDS = {\n",
    "        'statistical_parity':            0.05,\n",
    "        'equal_opportunity':             0.05,\n",
    "        'disparate_impact':              0.8\n",
    "    }\n",
    "    return check_thresholds(metrics, ALERT_THRESHOLDS)\n",
    "\n",
    "\n",
    "# 4) Clinical implementation\n",
    "def calibrate_risk_scores(scores: np.ndarray,\n",
    "                          races:  np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply group-specific thresholds to continuous risk scores,\n",
    "    returning a boolean array of who exceeds their threshold.\n",
    "    \"\"\"\n",
    "    thresholds = {\n",
    "        'Asian':           0.45,\n",
    "        'Hispanic':        0.47,\n",
    "        'Caucasian':       0.50,\n",
    "        'AfricanAmerican': 0.50,\n",
    "        'Other':           0.48\n",
    "    }\n",
    "    # Vectorized comparison\n",
    "    output = np.zeros_like(scores, dtype=bool)\n",
    "    for grp, thr in thresholds.items():\n",
    "        mask = (races == grp)\n",
    "        output[mask] = (scores[mask] > thr)\n",
    "    return output\n",
    "\n",
    "# 5) Treatment equity\n",
    "def identify_treatment_disparities(treatment_rates: pd.Series,\n",
    "                                  threshold:       float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a DataFrame of any subgroups whose treatment rate\n",
    "    differs by more than `threshold` from the overall mean.\n",
    "    \"\"\"\n",
    "    overall = treatment_rates.mean()\n",
    "    diff     = (treatment_rates - overall).abs()\n",
    "    return diff[diff > threshold].to_frame(name='difference')\n",
    "\n",
    "def monitor_treatment_equity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute diabetesMed usage rates by race×gender and flag\n",
    "    any subgroup that deviates by more than 5 percentage points.\n",
    "    \"\"\"\n",
    "    # Convert to 0/1\n",
    "    df['diabetesMed_flag'] = (df['diabetesMed'] == 'Yes').astype(int)\n",
    "    treatment_rates = df.groupby(['race', 'gender'])['diabetesMed_flag'].mean()\n",
    "    return identify_treatment_disparities(treatment_rates, threshold=0.05)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    df = pd.read_csv(\"../diabetesdatasets/diabetic_data.csv\")\n",
    "    \n",
    "    # Create binary outcome column \n",
    "    df[\"readmitted_binary\"] = df[\"readmitted\"].map({\"NO\": 0, \">30\": 1, \"<30\": 1})\n",
    "\n",
    "    # 1) Preprocessing weights\n",
    "    df = implement_balanced_sampling(df)\n",
    "    print(\"Sample weights attached:\", df[\"sample_weight\"].head(), \"\\n\")\n",
    "\n",
    "    # 2) Model development \n",
    "    X = df.drop(columns=[\"sample_weight\", \"readmitted\", \"readmitted_binary\"])\n",
    "    y = df[\"readmitted_binary\"]\n",
    "    sensitive = (df[\"race\"] == \"Black\").astype(int).values\n",
    "    cv, dummy_model = create_fair_model(X.values, y.values, sensitive)\n",
    "    print(\"Created StratifiedKFold:\", cv, \"\\n\")\n",
    "\n",
    "    # 3) Monitoring example\n",
    "    preds = np.random.randint(0, 2, size=len(df))\n",
    "    alerts = monitor_fairness_metrics(preds, y.values, sensitive)\n",
    "    print(\"Fairness alerts:\", alerts, \"\\n\")\n",
    "\n",
    "    # 4) Calibration example\n",
    "    risk_scores = np.random.rand(len(df))\n",
    "    decisions   = calibrate_risk_scores(risk_scores, df[\"race\"].values)\n",
    "    print(\"Calibrated decisions sample:\", decisions[:5], \"\\n\")\n",
    "\n",
    "    # 5) Treatment equity\n",
    "    disparities = monitor_treatment_equity(df)\n",
    "    print(\"Treatment disparities:\\n\", disparities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e295583-3e74-4d24-b786-d3891363e415",
   "metadata": {},
   "source": [
    "## Output\n",
    "This code implements a modular pipeline for assessing fairness and equity in clinical machine learning workflows using the diabetic patient dataset. It begins by assigning sampling weights to underrepresented racial groups in order to promote balanced data representation. A binary readmission label is derived and a stratified cross-validation object is created, preserving demographic balance across folds. Random predictions are then evaluated using fairness metrics, including statistical parity, equal opportunity, and disparate impact. All three metrics exceed predefined alert thresholds, indicating potential bias. Risk scores are calibrated using group-specific thresholds to simulate differential clinical decision rules. Finally, treatment equity is assessed by examining diabetes medication prescription rates across race and gender subgroups. The output reveals disparities in treatment for certain male and unknown-gender patients, especially those with missing or ambiguous race categories, suggesting areas where care delivery may be uneven. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8d0cc-140d-4e24-acfc-e4678ca77fb7",
   "metadata": {},
   "source": [
    "### Prompt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ab82a05-4d89-4214-a1fb-4ddcbc9cfb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "\n",
      "Analyzing demographic imbalances...\n",
      "\n",
      "Initial Imbalance Analysis:\n",
      "Race Imbalance Ratio: 118.72\n",
      "Gender Imbalance Ratio: 18236.00\n",
      "\n",
      "Race Distribution:\n",
      "race\n",
      "Caucasian          76099\n",
      "AfricanAmerican    19210\n",
      "Unknown             2273\n",
      "Hispanic            2037\n",
      "Other               1506\n",
      "Asian                641\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Gender Distribution:\n",
      "gender\n",
      "Female     54708\n",
      "Male       47055\n",
      "Unknown        3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying rebalancing techniques...\n",
      "\n",
      "Generating visualizations...\n",
      "\n",
      "Rebalancing Results:\n",
      "\n",
      "SMOTE:\n",
      "Sample size: 180818\n",
      "Class distribution:\n",
      "readmitted_binary\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Random Undersampling:\n",
      "Sample size: 22714\n",
      "Class distribution:\n",
      "readmitted_binary\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "SMOTEENN:\n",
      "Sample size: 116594\n",
      "Class distribution:\n",
      "readmitted_binary\n",
      "1    0.601566\n",
      "0    0.398434\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#llm code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # Load the diabetes dataset\n",
    "    df = pd.read_csv('../diabetesdatasets/diabetic_data.csv')\n",
    "    \n",
    "    # Convert readmitted to binary (30 days vs. >30/NO)\n",
    "    df['readmitted_binary'] = (df['readmitted'] == '<30').astype(int)\n",
    "    \n",
    "    # Handle missing values\n",
    "    df['race'] = df['race'].replace('?', 'Unknown')\n",
    "    df['gender'] = df['gender'].replace('Unknown/Invalid', 'Unknown')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_imbalance(df):\n",
    "    \"\"\"Analyze demographic imbalances in the dataset\"\"\"\n",
    "    imbalance_metrics = {}\n",
    "    \n",
    "    # Analyze race distribution\n",
    "    race_dist = df['race'].value_counts()\n",
    "    gender_dist = df['gender'].value_counts()\n",
    "    \n",
    "    # Calculate imbalance ratios\n",
    "    race_imbalance = race_dist.max() / race_dist.min()\n",
    "    gender_imbalance = gender_dist.max() / gender_dist.min()\n",
    "    \n",
    "    # Calculate readmission rates by demographic groups\n",
    "    race_readmission = df.groupby('race')['readmitted_binary'].mean()\n",
    "    gender_readmission = df.groupby('gender')['readmitted_binary'].mean()\n",
    "    \n",
    "    return {\n",
    "        'race_distribution': race_dist,\n",
    "        'gender_distribution': gender_dist,\n",
    "        'race_imbalance_ratio': race_imbalance,\n",
    "        'gender_imbalance_ratio': gender_imbalance,\n",
    "        'race_readmission_rates': race_readmission,\n",
    "        'gender_readmission_rates': gender_readmission\n",
    "    }\n",
    "\n",
    "def apply_rebalancing_techniques(df):\n",
    "    \"\"\"Apply different rebalancing techniques and evaluate results\"\"\"\n",
    "    # Prepare features and target\n",
    "    features = ['race', 'gender', 'age', 'time_in_hospital', 'num_lab_procedures', \n",
    "                'num_procedures', 'num_medications', 'number_diagnoses']\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_dict = {}\n",
    "    X = df[features].copy()\n",
    "    for col in ['race', 'gender', 'age']:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "        le_dict[col] = le\n",
    "    \n",
    "    y = df['readmitted_binary']\n",
    "    \n",
    "    # Apply different rebalancing techniques\n",
    "    techniques = {\n",
    "        'SMOTE': SMOTE(random_state=42),\n",
    "        'Random Undersampling': RandomUnderSampler(random_state=42),\n",
    "        'SMOTEENN': SMOTEENN(random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, technique in techniques.items():\n",
    "        X_resampled, y_resampled = technique.fit_resample(X, y)\n",
    "        \n",
    "        # Calculate class distribution after rebalancing\n",
    "        class_dist = pd.Series(y_resampled).value_counts(normalize=True)\n",
    "        \n",
    "        # Calculate demographic distribution after rebalancing\n",
    "        race_dist = pd.Series(le_dict['race'].inverse_transform(X_resampled['race'])).value_counts(normalize=True)\n",
    "        gender_dist = pd.Series(le_dict['gender'].inverse_transform(X_resampled['gender'])).value_counts(normalize=True)\n",
    "        \n",
    "        results[name] = {\n",
    "            'class_distribution': class_dist,\n",
    "            'race_distribution': race_dist,\n",
    "            'gender_distribution': gender_dist,\n",
    "            'sample_size': len(y_resampled)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_distributions(original_metrics, rebalancing_results):\n",
    "    \"\"\"Create visualizations of the distributions before and after rebalancing\"\"\"\n",
    "    # Plot original distributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Original race distribution\n",
    "    sns.barplot(x=original_metrics['race_distribution'].index, \n",
    "                y=original_metrics['race_distribution'].values,\n",
    "                ax=axes[0,0])\n",
    "    axes[0,0].set_title('Original Race Distribution')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Original gender distribution\n",
    "    sns.barplot(x=original_metrics['gender_distribution'].index,\n",
    "                y=original_metrics['gender_distribution'].values,\n",
    "                ax=axes[0,1])\n",
    "    axes[0,1].set_title('Original Gender Distribution')\n",
    "    \n",
    "    # Plot rebalancing results\n",
    "    technique_names = list(rebalancing_results.keys())\n",
    "    \n",
    "    # Compare race distributions after rebalancing\n",
    "    race_data = pd.DataFrame({name: results['race_distribution'] \n",
    "                             for name, results in rebalancing_results.items()})\n",
    "    race_data.plot(kind='bar', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Race Distribution After Rebalancing')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Compare gender distributions after rebalancing\n",
    "    gender_data = pd.DataFrame({name: results['gender_distribution']\n",
    "                               for name, results in rebalancing_results.items()})\n",
    "    gender_data.plot(kind='bar', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Gender Distribution After Rebalancing')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rebalancing_analysis.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = load_and_preprocess_data()\n",
    "    \n",
    "    # Analyze imbalances\n",
    "    print(\"\\nAnalyzing demographic imbalances...\")\n",
    "    imbalance_metrics = analyze_imbalance(df)\n",
    "    \n",
    "    # Print initial analysis\n",
    "    print(\"\\nInitial Imbalance Analysis:\")\n",
    "    print(f\"Race Imbalance Ratio: {imbalance_metrics['race_imbalance_ratio']:.2f}\")\n",
    "    print(f\"Gender Imbalance Ratio: {imbalance_metrics['gender_imbalance_ratio']:.2f}\")\n",
    "    print(\"\\nRace Distribution:\")\n",
    "    print(imbalance_metrics['race_distribution'])\n",
    "    print(\"\\nGender Distribution:\")\n",
    "    print(imbalance_metrics['gender_distribution'])\n",
    "    \n",
    "    # Apply rebalancing techniques\n",
    "    print(\"\\nApplying rebalancing techniques...\")\n",
    "    rebalancing_results = apply_rebalancing_techniques(df)\n",
    "    \n",
    "    # Plot results\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    plot_distributions(imbalance_metrics, rebalancing_results)\n",
    "    \n",
    "    # Print rebalancing results\n",
    "    print(\"\\nRebalancing Results:\")\n",
    "    for technique, results in rebalancing_results.items():\n",
    "        print(f\"\\n{technique}:\")\n",
    "        print(f\"Sample size: {results['sample_size']}\")\n",
    "        print(\"Class distribution:\")\n",
    "        print(results['class_distribution'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf8903-b8a5-4862-bb58-ff69b531cc7f",
   "metadata": {},
   "source": [
    "### output\n",
    "cleaning the diabetic readmission data, binarizing the “readmitted” field, and collapsing rare or missing race/gender labels into “Unknown.” It then measures how skewed the dataset is across race (a 118× gap between the largest and smallest groups) and gender (a 18 236× gap, driven by just three “Unknown” entries). Next, it applies three rebalancing strategies—SMOTE oversampling, random undersampling, and the hybrid SMOTE+ENN—and reports how each changes the positive/negative class mix and overall sample size. SMOTE expands the dataset to 180 818 examples with a perfect 50/50 split, undersampling shrinks it to 22 714 also at 50/50, and SMOTEENN yields 116 594 samples with about 60% positives and 40% negatives (removing noisy majority points). These outputs show how each technique can be used to trade off dataset size against class balance—SMOTE for maximum data, undersampling for simplicity, and SMOTEENN for a middle‐ground that also cleans borderline samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1617179-bb30-4ba9-a7e5-3bd1e56868c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. **Recommendations for Implementation**:\n",
    "\n",
    "#1. **Primary Approach**: Use SMOTE with these specific considerations:\n",
    "#   ```python\n",
    "smote = SMOTE(\n",
    "    random_state=42,\n",
    "    sampling_strategy='auto',\n",
    "    k_neighbors=5\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2987d51f-7e72-4c58-a536-00872e27d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTE, class distribution: Counter({0: 63286, 1: 63286})\n"
     ]
    }
   ],
   "source": [
    "#new code, some reused from previous code provided by LLM\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load and preprocess data\n",
    "df = load_and_preprocess_data()\n",
    "\n",
    "# Select features and encode categorical variables\n",
    "features = ['race', 'gender', 'age', 'time_in_hospital', 'num_lab_procedures', \n",
    "            'num_procedures', 'num_medications', 'number_diagnoses']\n",
    "le_dict = {}\n",
    "X = df[features].copy()\n",
    "for col in ['race', 'gender', 'age']:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    le_dict[col] = le\n",
    "\n",
    "y = df['readmitted_binary']\n",
    "\n",
    "# Split into train and test sets BEFORE applying SMOTE (prevents data leakage)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create and apply SMOTE with the specified parameters\n",
    "# This is the recommended usage based on LLM's code snippet:\n",
    "smote = SMOTE(random_state=42, sampling_strategy='auto', k_neighbors=5)\n",
    "\n",
    "# Fit SMOTE only on the training set!\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Optional: Check the new class distribution\n",
    "from collections import Counter\n",
    "print(\"After SMOTE, class distribution:\", Counter(y_train_smote))\n",
    "\n",
    "# Continue with model training using the rebalanced training data (X_train_smote, y_train_smote)\n",
    "# For example:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Step 6: Evaluate on the (untouched) test set\n",
    "y_pred = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b4869c-7e32-450f-917b-7e3553a241f4",
   "metadata": {},
   "source": [
    "# Output\n",
    "Even class distribution after SMOTE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
