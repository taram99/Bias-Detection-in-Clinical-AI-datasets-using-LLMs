{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1cade7-8a21-43fc-8248-bcc0ab72da85",
   "metadata": {},
   "source": [
    "## Claude programming code responses test for dissecting bias dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8cb7e5-46f4-4f96-9e4f-2007b8d00791",
   "metadata": {},
   "source": [
    "### Referred to this extra created file - prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22741242-a69e-4b2b-a3c4-0bedbbd46512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Analyzing group disparities...\n",
      "\n",
      "Original group representations:\n",
      "white_1: 55.5%\n",
      "white_0: 33.1%\n",
      "black_1: 7.6%\n",
      "black_0: 3.9%\n",
      "\n",
      "Risk score means by group:\n",
      "black_0: 5.56 (±9.26)\n",
      "black_1: 5.28 (±7.24)\n",
      "white_0: 4.39 (±5.52)\n",
      "white_1: 4.19 (±4.83)\n",
      "\n",
      "Enrollment rates by group:\n",
      "black_0: 1.5%\n",
      "black_1: 1.4%\n",
      "white_0: 1.0%\n",
      "white_1: 0.8%\n",
      "\n",
      "Calculating fair sampling weights...\n",
      "\n",
      "Applying fair resampling...\n",
      "\n",
      "Rebalancing results:\n",
      "Original total samples: 48784\n",
      "Rebalanced total samples: 72724\n",
      "\n",
      "Class distribution after rebalancing:\n",
      "{0: 48332, 1: 24392}\n",
      "\n",
      "Generating visualizations...\n",
      "\n",
      "Recommendations for Fair Model Training:\n",
      "1. Use the rebalanced dataset for initial model training\n",
      "2. Implement stratified cross-validation using both race and gender\n",
      "3. Apply the calculated sample weights during model training\n",
      "4. Monitor and validate performance separately for each demographic group\n",
      "5. Consider implementing fairness constraints in the model objective function\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('data_new.csv')\n",
    "\n",
    "def analyze_group_disparities(df):\n",
    "    # Create intersectional groups\n",
    "    df['group'] = df['race'] + '_' + df['dem_female'].astype(str)\n",
    "    \n",
    "    # Calculate various disparity metrics\n",
    "    disparities = {}\n",
    "    \n",
    "    # 1. Representation disparities\n",
    "    group_sizes = df['group'].value_counts()\n",
    "    total_samples = len(df)\n",
    "    disparities['representation'] = {\n",
    "        group: count/total_samples \n",
    "        for group, count in group_sizes.items()\n",
    "    }\n",
    "    \n",
    "    # 2. Risk score disparities\n",
    "    risk_stats = df.groupby('group')['risk_score_t'].agg(['mean', 'std'])\n",
    "    disparities['risk_scores'] = risk_stats.to_dict('index')\n",
    "    \n",
    "    # 3. Enrollment rate disparities\n",
    "    enrollment_rates = df.groupby('group')['program_enrolled_t'].mean()\n",
    "    disparities['enrollment_rates'] = enrollment_rates.to_dict()\n",
    "    \n",
    "    return disparities\n",
    "\n",
    "def calculate_sampling_weights(df, disparities):\n",
    "    # Calculate weights to balance both demographic and outcome representation\n",
    "    df['weight'] = 1.0\n",
    "    \n",
    "    # 1. Demographic balancing weights\n",
    "    max_group_size = df['group'].value_counts().max()\n",
    "    for group in df['group'].unique():\n",
    "        group_size = df[df['group'] == group].shape[0]\n",
    "        demographic_weight = max_group_size / group_size\n",
    "        df.loc[df['group'] == group, 'weight'] *= demographic_weight\n",
    "    \n",
    "    # 2. Outcome balancing weights\n",
    "    for group in df['group'].unique():\n",
    "        group_mask = df['group'] == group\n",
    "        enrolled_mask = df['program_enrolled_t'] == 1\n",
    "        \n",
    "        # Calculate positive class weight for this group\n",
    "        pos_samples = (group_mask & enrolled_mask).sum()\n",
    "        if pos_samples > 0:\n",
    "            pos_weight = 1 / (pos_samples / group_mask.sum())\n",
    "            df.loc[group_mask & enrolled_mask, 'weight'] *= pos_weight\n",
    "    \n",
    "    return df['weight']\n",
    "\n",
    "def apply_fair_resampling(df, weights):\n",
    "    # Prepare data for resampling\n",
    "    X = df[['risk_score_t']]  # Features\n",
    "    y = df['program_enrolled_t']  # Target\n",
    "    groups = df['group']  # Group membership\n",
    "    \n",
    "    # Store original distributions\n",
    "    original_dist = {\n",
    "        'class': dict(Counter(y)),\n",
    "        'group': dict(Counter(groups))\n",
    "    }\n",
    "    \n",
    "    # Apply SMOTE with custom sampling strategy based on weights\n",
    "    sampling_strategy = {1: int(len(df) * 0.5)}  # Target 50-50 class balance\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    \n",
    "    # Resample\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Create resampled dataframe\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=['risk_score_t'])\n",
    "    df_resampled['program_enrolled_t'] = y_resampled\n",
    "    \n",
    "    return df_resampled, original_dist\n",
    "\n",
    "def plot_rebalancing_effects(original_dist, df_resampled):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot class distribution changes\n",
    "    class_dist = {\n",
    "        'Original': original_dist['class'],\n",
    "        'Rebalanced': dict(Counter(df_resampled['program_enrolled_t']))\n",
    "    }\n",
    "    \n",
    "    # Class distribution plot\n",
    "    class_df = pd.DataFrame(class_dist).T\n",
    "    class_df.plot(kind='bar', ax=ax1)\n",
    "    ax1.set_title('Class Distribution Before and After Rebalancing')\n",
    "    ax1.set_ylabel('Number of Samples')\n",
    "    \n",
    "    # Risk score distribution plot\n",
    "    ax2.hist(df_resampled['risk_score_t'], bins=50, alpha=0.5, label='Rebalanced')\n",
    "    ax2.set_title('Risk Score Distribution After Rebalancing')\n",
    "    ax2.set_xlabel('Risk Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fair_rebalancing_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main execution\n",
    "print(\"\\nAnalyzing group disparities...\")\n",
    "disparities = analyze_group_disparities(df)\n",
    "\n",
    "print(\"\\nOriginal group representations:\")\n",
    "for group, prop in disparities['representation'].items():\n",
    "    print(f\"{group}: {prop:.1%}\")\n",
    "\n",
    "print(\"\\nRisk score means by group:\")\n",
    "for group, stats in disparities['risk_scores'].items():\n",
    "    print(f\"{group}: {stats['mean']:.2f} (±{stats['std']:.2f})\")\n",
    "\n",
    "print(\"\\nEnrollment rates by group:\")\n",
    "for group, rate in disparities['enrollment_rates'].items():\n",
    "    print(f\"{group}: {rate:.1%}\")\n",
    "\n",
    "print(\"\\nCalculating fair sampling weights...\")\n",
    "weights = calculate_sampling_weights(df, disparities)\n",
    "\n",
    "print(\"\\nApplying fair resampling...\")\n",
    "df_resampled, original_dist = apply_fair_resampling(df, weights)\n",
    "\n",
    "print(\"\\nRebalancing results:\")\n",
    "print(f\"Original total samples: {len(df)}\")\n",
    "print(f\"Rebalanced total samples: {len(df_resampled)}\")\n",
    "print(\"\\nClass distribution after rebalancing:\")\n",
    "print(dict(Counter(df_resampled['program_enrolled_t'])))\n",
    "\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "plot_rebalancing_effects(original_dist, df_resampled)\n",
    "\n",
    "print(\"\\nRecommendations for Fair Model Training:\")\n",
    "print(\"1. Use the rebalanced dataset for initial model training\")\n",
    "print(\"2. Implement stratified cross-validation using both race and gender\")\n",
    "print(\"3. Apply the calculated sample weights during model training\")\n",
    "print(\"4. Monitor and validate performance separately for each demographic group\")\n",
    "print(\"5. Consider implementing fairness constraints in the model objective function\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572c925-c8e8-473a-a839-44ac421bfbd5",
   "metadata": {},
   "source": [
    "## Output\n",
    " Initial analysis revealed imbalances in group representation, risk scores, and enrollment rates, with white individuals making up the majority and black individuals underrepresented. Risk scores and enrollment rates also varied slightly across these groups. To mitigate these disparities, fair sampling weights were calculated and applied, increasing the dataset size from 48,784 to 72,724 samples. The resulting class distribution became more balanced, enabling a fairer foundation for model training. Recommendations include using the rebalanced data, applying stratified cross-validation across race and gender,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a97b0d7-1ebb-469b-a055-86bb2a0c1167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9902634006354413\n",
      "ROC-AUC: 0.8709001078560612\n"
     ]
    }
   ],
   "source": [
    "#Old code given by llm\n",
    "#1. **Two-Stage Rebalancing Approach:**\n",
    "# Stage 1: Demographic balancing\n",
    "#demographic_weights = max_group_size / group_size\n",
    "\n",
    "# Stage 2: Outcome balancing within groups\n",
    "#outcome_weights = 1 / (positive_samples / group_size)\n",
    "\n",
    "# Combined weights\n",
    "#final_weights = demographic_weights * outcome_weights\n",
    "\n",
    "#2. **Stratified Sampling Implementation:**\n",
    "# Stratification by both race and gender\n",
    "#stratify_columns = ['race', 'dem_female']\n",
    "#train_test_split(stratify=df[stratify_columns])\n",
    "\n",
    "\n",
    "#New code:\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv(\"data_new.csv\")\n",
    "\n",
    "# 2. Build intersectional groups\n",
    "df['group'] = df['race'] + '_' + df['dem_female'].astype(str)\n",
    "\n",
    "# 3. Compute two‐stage rebalancing weights\n",
    "group_counts      = df['group'].value_counts()\n",
    "max_group_size    = group_counts.max()\n",
    "demographic_weights = df['group'].map(lambda g: max_group_size / group_counts[g])\n",
    "\n",
    "pos_counts      = df[df['program_enrolled_t'] == 1]['group'].value_counts()\n",
    "outcome_weights = df['group'].map(\n",
    "    lambda g: 1.0 / (pos_counts[g] / group_counts[g]) if g in pos_counts else 1.0\n",
    ")\n",
    "\n",
    "df['final_weight'] = demographic_weights * outcome_weights\n",
    "\n",
    "# 4. Create strata for stratified splitting\n",
    "df['strata'] = df['group']  # preserves race+gender mix\n",
    "\n",
    "# 5. Define X, y, and weight vector w\n",
    "y = df['program_enrolled_t']\n",
    "X = df.drop(columns=['program_enrolled_t', 'group', 'final_weight', 'strata'])\n",
    "w = df['final_weight']\n",
    "\n",
    "# 6. Stratified train/test split\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "    X, y, w, test_size=0.20, stratify=df['strata'], random_state=42\n",
    ")\n",
    "\n",
    "# 7. Preprocessing pipeline (impute → encode/scale)\n",
    "categorical_cols = ['race', 'dem_female']\n",
    "numerical_cols   = [c for c in X_train.columns if c not in categorical_cols]\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot',  OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler',   StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_pipeline, categorical_cols),\n",
    "    ('num', num_pipeline, numerical_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed  = preprocessor.transform(X_test)\n",
    "\n",
    "# 8. Train the model with sample weights\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(\n",
    "    X_train_processed,\n",
    "    y_train,\n",
    "    sample_weight=w_train\n",
    ")\n",
    "\n",
    "# 9. Evaluate overall performance\n",
    "y_pred  = model.predict(X_test_processed)\n",
    "y_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"ROC-AUC:\",  roc_auc_score(y_test, y_proba))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1207d1-f8e5-46c3-8a7e-0f2e29bdeba2",
   "metadata": {},
   "source": [
    "## Comment\n",
    "Changes made to original code:\n",
    "\n",
    "Expanded weight computation: Replaced the simple placeholders (demographic_weights = max_group_size / group_size, outcome_weights = 1 / (…)) with actual Pandas mappings that calculate per-row demographic_weights, outcome_weights, and multiply into df['final_weight'].\n",
    "\n",
    "Built explicit “strata” column: Added df['strata'] = df['group'] so train_test_split(..., stratify=...) actually preserves race+gender mixes.\n",
    "\n",
    "Defined feature/target/weight arrays: Specified y, X, and w = df['final_weight'] instead of leaving them implicit.\n",
    "\n",
    "Passed weights into split: Changed train_test_split to accept X, y, w and return w_train, w_test aligned with the splits.\n",
    "\n",
    "Added full preprocessing: Introduced a ColumnTransformer with separate pipelines for imputing/scaling numerics and imputing/one-hot encoding categoricals—without this, model.fit would error on NaNs or strings.\n",
    "\n",
    "Instantiated and trained a real model: Created a RandomForestClassifier and called fit(X_train_processed, y_train, sample_weight=w_train), rather than leaving model.fit(...) undefined.\n",
    "\n",
    "Evaluation step: Included predict, predict_proba, and printed overall accuracy and ROC-AUC, so you get immediate feedback instead of a silent placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0aff8f3-3530-443b-aa9b-834e1b5bf378",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3833615725.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    # Ensure balanced representation in each fold\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "#2. **Feature Engineering:**\n",
    "# Add interaction terms to capture intersectional effects\n",
    "df['race_gender_interaction'] = df['race'] + '_' + df['dem_female']\n",
    "\n",
    "#3. **Validation Strategy:**\n",
    "# K-fold cross-validation with multiple stratification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "for train_idx, val_idx in skf.split(X, df[stratify_columns]):\n",
    "    # Ensure balanced representation in each fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc95ed-331c-4f32-ac92-7425ca57791a",
   "metadata": {},
   "source": [
    "## Comment\n",
    "Did not finish code, only gave some recommendation. Not clear what to do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b289a0e6-7a95-4f90-9fc2-40d4dfad57e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#**3. Model Training Recommendations:**\n",
    "#A. **Fairness-Aware Training:**\n",
    "#1. **Sample Weights:**\n",
    "# Apply calculated weights during model training\n",
    "model.fit(\n",
    "    X_train_processed,\n",
    "    y_train,\n",
    "    sample_weight=w_train\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37ae2d-56fe-46ab-8fb1-f429cc9d1124",
   "metadata": {},
   "source": [
    "## Comment\n",
    "Only needed to change variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbe8d56d-a235-46ed-a1fe-810977d5346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fair loss (logloss + λ·disparity): 0.0736\n",
      "\n",
      "Per-group performance:\n",
      " - black_0:  acc=0.987,  AUC=0.770\n",
      " - black_1:  acc=0.985,  AUC=0.914\n",
      " - white_0:  acc=0.989,  AUC=0.846\n",
      " - white_1:  acc=0.992,  AUC=0.886\n",
      "\n",
      "Demographic parity gap (white_1 vs white_0): 0.000\n"
     ]
    }
   ],
   "source": [
    "#Old code provided by LLM\n",
    "#2. **Custom Loss Function:**\n",
    "#def fair_loss(y_true, y_pred, group_membership):\n",
    "    # Base loss\n",
    "#    base_loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Fairness penalty\n",
    "#    fairness_penalty = calculate_group_disparity(y_true, y_pred, group_membership)\n",
    "    \n",
    "#    return base_loss + lambda_fairness * fairness_penalty\n",
    "\n",
    "\n",
    "#B. **Monitoring and Validation:**\n",
    "#1. **Group-Specific Metrics:**\n",
    "#for group in groups:\n",
    "#    group_metrics = evaluate_model(model, X[group_mask], y[group_mask])\n",
    "#    monitor_group_performance(group, group_metrics)\n",
    "\n",
    "#2. **Fairness Constraints:**\n",
    "# Implement demographic parity constraint\n",
    "#demographic_parity_diff = abs(group1_pred_rate - group2_pred_rate)\n",
    "#assert demographic_parity_diff <= threshold\n",
    "\n",
    "#New code:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score\n",
    "\n",
    "# — assume you already did —\n",
    "# df = pd.read_csv(\"data_new.csv\")\n",
    "# df['group'] = df['race'] + '_' + df['dem_female'].astype(str)\n",
    "# X_train_processed, X_test_processed, y_train, y_test, model have been defined\n",
    "\n",
    "# For convenience:\n",
    "groups_test = df.loc[y_test.index, 'group'].values\n",
    "probs_test  = model.predict_proba(X_test_processed)[:,1]\n",
    "preds_test  = (probs_test > 0.5).astype(int)\n",
    "\n",
    "# Hyperparameters\n",
    "lambda_fairness = 0.5\n",
    "threshold       = 0.05  # max allowed parity gap\n",
    "\n",
    "# 1. Custom loss & helper\n",
    "def calculate_group_disparity(y_true, y_pred, groups):\n",
    "    \"\"\"Max absolute difference between group-wise mean(pred) and mean(true).\"\"\"\n",
    "    disparities = []\n",
    "    for g in np.unique(groups):\n",
    "        mask = (groups == g)\n",
    "        disparities.append(abs(y_pred[mask].mean() - y_true[mask].mean()))\n",
    "    return max(disparities)\n",
    "\n",
    "def fair_loss(y_true, y_pred, groups, lam):\n",
    "    \"\"\"Log‐loss plus fairness penalty.\"\"\"\n",
    "    base = log_loss(y_true, y_pred)\n",
    "    pen  = calculate_group_disparity(y_true, y_pred, groups)\n",
    "    return base + lam * pen\n",
    "\n",
    "# Compute and print the fair loss on your test set\n",
    "fl = fair_loss(y_test.values, probs_test, groups_test, lambda_fairness)\n",
    "print(f\"Fair loss (logloss + λ·disparity): {fl:.4f}\")\n",
    "\n",
    "# 2. Monitoring & group-specific metrics\n",
    "print(\"\\nPer-group performance:\")\n",
    "for g in np.unique(groups_test):\n",
    "    m = (groups_test == g)\n",
    "    acc = accuracy_score(y_test[m], preds_test[m])\n",
    "    auc = roc_auc_score(y_test[m], probs_test[m])\n",
    "    print(f\" - {g}:  acc={acc:.3f},  AUC={auc:.3f}\")\n",
    "\n",
    "# 3. Demographic-parity check between the two largest groups\n",
    "grp_counts = pd.Series(groups_test).value_counts()\n",
    "g1, g2 = grp_counts.index[:2]    # top two\n",
    "rate1 = preds_test[groups_test==g1].mean()\n",
    "rate2 = preds_test[groups_test==g2].mean()\n",
    "diff  = abs(rate1 - rate2)\n",
    "print(f\"\\nDemographic parity gap ({g1} vs {g2}): {diff:.3f}\")\n",
    "assert diff <= threshold, f\"Parity gap {diff:.3f} exceeds threshold {threshold}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5163e6c-4b6b-4735-83fb-0d3cf4881df8",
   "metadata": {},
   "source": [
    "## Output\n",
    "Fair-loss score of 0.0736 combines log-loss and the fairness penalty, so lower is better, showing the model is both accurate and balanced across groups. All four subgroups have high accuracy (98.5–99.2 %) and strong ROC AUCs, though it’s weakest on Black men (AUC 0.770) versus Black women (0.914) or White women (0.886). The demographic-parity gap between White women and White men is 0.000, meaning they receive identical positive prediction rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009cdc29-69de-4d6b-a0eb-0f06aa009fed",
   "metadata": {},
   "source": [
    "### Prompt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a8787-3603-44c8-8601-8311650232a8",
   "metadata": {},
   "source": [
    "### Fairness metric file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "358e403c-086f-41f3-9dd1-cb19dd4b5980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fairness Metrics Analysis\n",
      "==================================================\n",
      "\n",
      "Reference group: white_1\n",
      "\n",
      "Detailed Metrics by Group:\n",
      "\n",
      "white_0 (n=16125):\n",
      "  prediction_rate: 0.502\n",
      "  actual_rate: 0.010\n",
      "  false_positive_rate: 0.497\n",
      "  false_negative_rate: 0.006\n",
      "  true_positive_rate: 0.994\n",
      "  positive_predictive_value: 0.020\n",
      "\n",
      "white_1 (n=27077):\n",
      "  prediction_rate: 0.493\n",
      "  actual_rate: 0.008\n",
      "  false_positive_rate: 0.489\n",
      "  false_negative_rate: 0.024\n",
      "  true_positive_rate: 0.976\n",
      "  positive_predictive_value: 0.015\n",
      "\n",
      "black_1 (n=3686):\n",
      "  prediction_rate: 0.540\n",
      "  actual_rate: 0.014\n",
      "  false_positive_rate: 0.533\n",
      "  false_negative_rate: 0.000\n",
      "  true_positive_rate: 1.000\n",
      "  positive_predictive_value: 0.026\n",
      "\n",
      "black_0 (n=1896):\n",
      "  prediction_rate: 0.483\n",
      "  actual_rate: 0.015\n",
      "  false_positive_rate: 0.476\n",
      "  false_negative_rate: 0.071\n",
      "  true_positive_rate: 0.929\n",
      "  positive_predictive_value: 0.028\n",
      "\n",
      "Fairness Differences (compared to reference group):\n",
      "\n",
      "white_0:\n",
      "  equal_opportunity_diff: 0.018\n",
      "  demographic_parity_diff: 0.009\n",
      "  fpr_diff: 0.008\n",
      "  ppv_diff: 0.004\n",
      "\n",
      "black_1:\n",
      "  equal_opportunity_diff: 0.024\n",
      "  demographic_parity_diff: 0.047\n",
      "  fpr_diff: 0.044\n",
      "  ppv_diff: 0.010\n",
      "\n",
      "black_0:\n",
      "  equal_opportunity_diff: -0.048\n",
      "  demographic_parity_diff: -0.010\n",
      "  fpr_diff: -0.013\n",
      "  ppv_diff: 0.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_5484/484503456.py:94: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(groups, rotation=45, ha='right')\n",
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_5484/484503456.py:94: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(groups, rotation=45, ha='right')\n",
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_5484/484503456.py:94: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(groups, rotation=45, ha='right')\n",
      "/var/folders/cy/6qhdwjmx7sd3qq25c4m0mf300000gn/T/ipykernel_5484/484503456.py:94: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(groups, rotation=45, ha='right')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('data_new.csv')\n",
    "\n",
    "def calculate_fairness_metrics(df, group_columns):\n",
    "    # Convert risk score to binary prediction (using median as threshold)\n",
    "    median_risk = df['risk_score_t'].median()\n",
    "    df['predicted_high_risk'] = (df['risk_score_t'] > median_risk).astype(int)\n",
    "    \n",
    "    # Use program enrollment as actual outcome\n",
    "    df['actual_outcome'] = df['program_enrolled_t']\n",
    "    \n",
    "    # Calculate metrics by group\n",
    "    metrics = {}\n",
    "    \n",
    "    # Create group combinations\n",
    "    df['group_key'] = df[group_columns].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "    groups = df['group_key'].unique()\n",
    "    \n",
    "    for group in groups:\n",
    "        group_mask = df['group_key'] == group\n",
    "        group_data = df[group_mask]\n",
    "        \n",
    "        if len(group_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Prediction rates\n",
    "        pred_rate = group_data['predicted_high_risk'].mean()\n",
    "        actual_rate = group_data['actual_outcome'].mean()\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(\n",
    "            group_data['actual_outcome'],\n",
    "            group_data['predicted_high_risk']\n",
    "        ).ravel()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value\n",
    "        \n",
    "        # Calculate equal opportunity difference (true positive rate difference)\n",
    "        # and demographic parity difference\n",
    "        metrics[group] = {\n",
    "            'group_size': len(group_data),\n",
    "            'prediction_rate': pred_rate,\n",
    "            'actual_rate': actual_rate,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'true_positive_rate': tpr,\n",
    "            'positive_predictive_value': ppv\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_fairness_differences(metrics):\n",
    "    # Find reference group (usually the majority group)\n",
    "    reference_group = max(metrics.items(), key=lambda x: x[1]['group_size'])[0]\n",
    "    ref_metrics = metrics[reference_group]\n",
    "    \n",
    "    differences = {}\n",
    "    for group in metrics:\n",
    "        if group != reference_group:\n",
    "            group_metrics = metrics[group]\n",
    "            \n",
    "            # Calculate differences in key metrics\n",
    "            differences[group] = {\n",
    "                'equal_opportunity_diff': group_metrics['true_positive_rate'] - ref_metrics['true_positive_rate'],\n",
    "                'demographic_parity_diff': group_metrics['prediction_rate'] - ref_metrics['prediction_rate'],\n",
    "                'fpr_diff': group_metrics['false_positive_rate'] - ref_metrics['false_positive_rate'],\n",
    "                'ppv_diff': group_metrics['positive_predictive_value'] - ref_metrics['positive_predictive_value']\n",
    "            }\n",
    "    \n",
    "    return differences, reference_group\n",
    "\n",
    "def plot_fairness_metrics(metrics, group_columns):\n",
    "    groups = list(metrics.keys())\n",
    "    metrics_to_plot = ['prediction_rate', 'false_positive_rate', 'false_negative_rate', 'true_positive_rate']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        values = [m[metric] for m in metrics.values()]\n",
    "        ax = axes[idx]\n",
    "        ax.bar(groups, values)\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "        ax.set_xticklabels(groups, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fairness_metrics.png')\n",
    "    plt.close()\n",
    "\n",
    "# Calculate metrics for different demographic intersections\n",
    "group_columns = ['race', 'dem_female']  # Adding gender intersection\n",
    "metrics = calculate_fairness_metrics(df, group_columns)\n",
    "differences, reference_group = calculate_fairness_differences(metrics)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nFairness Metrics Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nReference group: {reference_group}\")\n",
    "\n",
    "print(\"\\nDetailed Metrics by Group:\")\n",
    "for group, group_metrics in metrics.items():\n",
    "    print(f\"\\n{group} (n={group_metrics['group_size']}):\")\n",
    "    for metric, value in group_metrics.items():\n",
    "        if metric != 'group_size':\n",
    "            print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nFairness Differences (compared to reference group):\")\n",
    "for group, diff_metrics in differences.items():\n",
    "    print(f\"\\n{group}:\")\n",
    "    for metric, value in diff_metrics.items():\n",
    "        # Add interpretation threshold\n",
    "        threshold = 0.1  # 10% difference threshold\n",
    "        interpretation = \"\"\n",
    "        if abs(value) > threshold:\n",
    "            interpretation = \" [SIGNIFICANT DISPARITY]\"\n",
    "        print(f\"  {metric}: {value:.3f}{interpretation}\")\n",
    "\n",
    "# Create visualizations\n",
    "plot_fairness_metrics(metrics, group_columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168f167-4fb8-4ffb-a0a8-8f521b7d828e",
   "metadata": {},
   "source": [
    "## Output\n",
    "The analysis uses each group’s median risk score to make a binary “high‐risk” prediction and then compares that to actual program enrollment. White women (white_1) serve as the reference.\n",
    "\n",
    "Overall behavior: All four groups have very low enrollment rates (~0.8–1.5%) and the model predicts “high risk” for about 48–54% of cases, so it makes many false positives.\n",
    "\n",
    "True-positive rates (TPR) are highest for Black women (black_1: 1.000) and lowest for Black men (black_0: 0.929), so Black men are the only group whose equal-opportunity diff (–0.048) dips below the reference.\n",
    "\n",
    "Demographic-parity diffs (difference in how often the model flags “high risk”) range from –0.010 (black_0) to +0.047 (black_1), meaning Black women are flagged slightly more often than White women, and Black men slightly less.\n",
    "\n",
    "False-positive rate (FPR) differences track these parity gaps (±0.008–0.044), and positive-predictive values (PPV) are all under 3%.\n",
    "\n",
    "Because none of these differences exceed the 10% threshold, the model meets the chosen fairness guardrails overall. The largest shortfall is that Black men see about a 4.8% lower true-positive rate than White women, suggesting you might tune the model further if equal opportunity for that subgroup is a priority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a346b5c-4134-4bfe-b87f-cda3cde0f606",
   "metadata": {},
   "source": [
    "### Prompt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e104616b-c771-4800-a4cc-713e725c0aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /Users/taramac-lean/anaconda3/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/taramac-lean/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/taramac-lean/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/taramac-lean/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/taramac-lean/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/taramac-lean/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (2.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c22c1-beed-498b-994b-e242c027a0f1",
   "metadata": {},
   "source": [
    "### Evaluation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "204d60f2-8396-4f02-b2ec-6d59368d81c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "\n",
      "Analyzing group distributions...\n",
      "\n",
      "Group Statistics:\n",
      "          size  proportion  prediction_rate  enrollment_rate\n",
      "group                                                       \n",
      "black_0   1896    3.886520         5.555866         0.014768\n",
      "black_1   3686    7.555756         5.281573         0.013836\n",
      "white_0  16125   33.053870         4.389017         0.010047\n",
      "white_1  27077   55.503854         4.194229         0.007793\n",
      "\n",
      "Evaluating imbalance severity...\n",
      "\n",
      "Imbalance Metrics:\n",
      "imbalance_ratio: 14.281118143459915\n",
      "coefficient_of_variation: 0.9648296832362823\n",
      "size_range: 1,896 - 27,077\n",
      "\n",
      "Preparing data for rebalancing...\n",
      "\n",
      "Applying rebalancing techniques...\n",
      "\n",
      "Rebalancing Results:\n",
      "original: {0: 48332, 1: 452}\n",
      "smote: {0: 48332, 1: 48332}\n",
      "undersampling: {0: 452, 1: 452}\n",
      "smoteenn: {0: 46402, 1: 38024}\n",
      "\n",
      "Generating visualizations...\n",
      "\n",
      "Recommendations:\n",
      "- Severe imbalance detected. Consider using SMOTEENN for balanced performance.\n",
      "- Implement cost-sensitive learning or class weights.\n",
      "- Consider collecting more data for minority groups.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading and preparing data...\")\n",
    "df = pd.read_csv('data_new.csv')\n",
    "\n",
    "def analyze_group_distributions(df):\n",
    "    # Create intersectional groups\n",
    "    df['group'] = df['race'] + '_' + df['dem_female'].astype(str)\n",
    "    \n",
    "    # Calculate group sizes and proportions\n",
    "    group_stats = pd.DataFrame({\n",
    "        'size': df.groupby('group').size(),\n",
    "        'proportion': df.groupby('group').size() / len(df) * 100\n",
    "    })\n",
    "    \n",
    "    # Calculate prediction rates per group\n",
    "    group_stats['prediction_rate'] = df.groupby('group')['risk_score_t'].mean()\n",
    "    group_stats['enrollment_rate'] = df.groupby('group')['program_enrolled_t'].mean()\n",
    "    \n",
    "    return group_stats\n",
    "\n",
    "def plot_group_distributions(group_stats):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot group sizes\n",
    "    group_stats['size'].plot(kind='bar', ax=ax1)\n",
    "    ax1.set_title('Group Sizes')\n",
    "    ax1.set_ylabel('Number of Samples')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot rates\n",
    "    rates = group_stats[['prediction_rate', 'enrollment_rate']]\n",
    "    rates.plot(kind='bar', ax=ax2)\n",
    "    ax2.set_title('Prediction and Enrollment Rates by Group')\n",
    "    ax2.set_ylabel('Rate')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('group_distributions.png')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_imbalance_severity(group_stats):\n",
    "    # Calculate imbalance metrics\n",
    "    max_size = group_stats['size'].max()\n",
    "    min_size = group_stats['size'].min()\n",
    "    imbalance_ratio = max_size / min_size\n",
    "    \n",
    "    # Calculate coefficient of variation\n",
    "    cv = group_stats['size'].std() / group_stats['size'].mean()\n",
    "    \n",
    "    return {\n",
    "        'imbalance_ratio': imbalance_ratio,\n",
    "        'coefficient_of_variation': cv,\n",
    "        'size_range': f\"{min_size:,} - {max_size:,}\"\n",
    "    }\n",
    "\n",
    "def prepare_data_for_rebalancing(df):\n",
    "    # Create features and target\n",
    "    X = df[['risk_score_t']]  # Using risk score as feature for demonstration\n",
    "    y = df['program_enrolled_t']  # Program enrollment as target\n",
    "    groups = df['race'] + '_' + df['dem_female'].astype(str)\n",
    "    \n",
    "    return X, y, groups\n",
    "\n",
    "def apply_rebalancing_techniques(X, y, groups):\n",
    "    results = {}\n",
    "    \n",
    "    # Original distribution\n",
    "    results['original'] = dict(Counter(y))\n",
    "    \n",
    "    # SMOTE oversampling\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y)\n",
    "    results['smote'] = dict(Counter(y_smote))\n",
    "    \n",
    "    # Random undersampling\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_rus, y_rus = rus.fit_resample(X, y)\n",
    "    results['undersampling'] = dict(Counter(y_rus))\n",
    "    \n",
    "    # Combined SMOTE+ENN\n",
    "    smoteenn = SMOTEENN(random_state=42)\n",
    "    X_smoteenn, y_smoteenn = smoteenn.fit_resample(X, y)\n",
    "    results['smoteenn'] = dict(Counter(y_smoteenn))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_rebalancing_results(rebalancing_results):\n",
    "    techniques = list(rebalancing_results.keys())\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(techniques))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Plot class distributions for each technique\n",
    "    for i, technique in enumerate(techniques):\n",
    "        counts = rebalancing_results[technique]\n",
    "        ax.bar(x[i], counts[0], width, label='Class 0', color='skyblue')\n",
    "        ax.bar(x[i], counts[1], width, bottom=counts[0], label='Class 1' if i == 0 else \"\", color='lightcoral')\n",
    "    \n",
    "    ax.set_ylabel('Number of Samples')\n",
    "    ax.set_title('Class Distribution After Different Rebalancing Techniques')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(techniques, rotation=45)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rebalancing_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main analysis\n",
    "print(\"\\nAnalyzing group distributions...\")\n",
    "group_stats = analyze_group_distributions(df)\n",
    "print(\"\\nGroup Statistics:\")\n",
    "print(group_stats)\n",
    "\n",
    "print(\"\\nEvaluating imbalance severity...\")\n",
    "imbalance_metrics = evaluate_imbalance_severity(group_stats)\n",
    "print(\"\\nImbalance Metrics:\")\n",
    "for metric, value in imbalance_metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "print(\"\\nPreparing data for rebalancing...\")\n",
    "X, y, groups = prepare_data_for_rebalancing(df)\n",
    "\n",
    "print(\"\\nApplying rebalancing techniques...\")\n",
    "rebalancing_results = apply_rebalancing_techniques(X, y, groups)\n",
    "print(\"\\nRebalancing Results:\")\n",
    "for technique, counts in rebalancing_results.items():\n",
    "    print(f\"{technique}: {counts}\")\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "plot_group_distributions(group_stats)\n",
    "plot_rebalancing_results(rebalancing_results)\n",
    "\n",
    "# Recommendations based on imbalance ratio\n",
    "print(\"\\nRecommendations:\")\n",
    "if imbalance_metrics['imbalance_ratio'] > 10:\n",
    "    print(\"- Severe imbalance detected. Consider using SMOTEENN for balanced performance.\")\n",
    "    print(\"- Implement cost-sensitive learning or class weights.\")\n",
    "    print(\"- Consider collecting more data for minority groups.\")\n",
    "elif imbalance_metrics['imbalance_ratio'] > 3:\n",
    "    print(\"- Moderate imbalance detected. Consider using SMOTE for minority class oversampling.\")\n",
    "    print(\"- Evaluate performance with and without rebalancing.\")\n",
    "else:\n",
    "    print(\"- Mild imbalance. Standard techniques may be sufficient.\")\n",
    "    print(\"- Monitor performance across groups during training.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39f96f-0fa9-4117-a484-5b2e9b6e690b",
   "metadata": {},
   "source": [
    "### Output\n",
    "The results show a highly skewed dataset both by demographic group and by outcome. Intersectionally, White women make up 55.5% of the data while Black men are with only 3.9%, and the largest group is more than 14× bigger than the smallest (imbalance ratio ≈14.3, CV≈0.96). In the raw outcome distribution there are 48,332 negatives versus just 452 positives a 99:1 split.\n",
    "\n",
    "When applying SMOTE, oversample until each class has 48,332 examples; with random undersampling shrink both classes to 452; and with SMOTE+ENN end up with about 46,402 negatives and 38,024 positives, a much more balanced compromise. Because the imbalance ratio exceeds 10, the code recommends SMOTEENN for best class balance, using cost-sensitive learning or class weights in the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
