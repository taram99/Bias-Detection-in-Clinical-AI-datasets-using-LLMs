# Bias-Detection-in-Clinical-AI-datasets-using-LLMs

This repository accompanies my Bachelor's thesis, Bias detection in clinical AI datasets using LLMs, submitted to the Vrije Universiteit Amsterdam.
The thesis explores how large language models can audit healthcare datasets for fairness before model training and support rebalancing strategies. Using two datasets, manual and mdoel-based audits were compared across GPT-4.1, Gemini 2.5 Pro and Claude 3.5 Sonnet. The models successfully identified demographic imbalances and recommended fairness metrics and corrective actions. 

# Repository contents
## Datasets 
This repository includes two datasets:
-data_new.csv
-diabetes_data.csv

## Manual Audits
-Manual audit for comorbidities dataset (data_new.csv)
-Manual audit for diabetes dataset (diabetes_data.csv)

These manual audits include:
-Group representation and outcome rates across race, gender, and age group
-Calculation of fairness metrics (Statistical Parity Difference and Disparate Impact Ratio)
-Missing data analysis by subgroup
-Intersectional analysis
-Recommendation rebalancing strategies

## Open coding experiment
This document included responses for the model based audit that are highlighted with codes such as "Literature" or "Fairness Awareness" to indicate the meaning of the text for qualitative coding analysis.

## Programming code analysis
-All notebooks contain programming code generated by LLMs, along with an evaluation if the code runs as it is or if it requires modifications. 

⚠️ Note: For some prompts and some LLMs tested, no code was returned by the model. These cases are documented accordingly in the results in my thesis.

