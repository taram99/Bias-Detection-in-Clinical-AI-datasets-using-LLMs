{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6197ae-35bc-45be-bdbf-0b416d47467a",
   "metadata": {},
   "source": [
    "## Gemini programming code responses test for comorbidities dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae3ceb-2f5b-4f4e-a393-db4a339aa72c",
   "metadata": {},
   "source": [
    "# Prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e138da7-bb1c-4bce-8009-0ee1aa80a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9f7909b-9389-4639-83a9-5c625b69e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('data_new.csv', nrows=5); print(df.columns.tolist()); print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71581909-abe5-4598-8704-1ec912c0e9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape: (48784, 160)\n",
      "\n",
      "Race Distribution (Counts):\n",
      "race\n",
      "white    43202\n",
      "black     5582\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Race Distribution (Percentage):\n",
      "race\n",
      "white    88.557724\n",
      "black    11.442276\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 2) Double-check the shape\n",
    "print(\"Full dataset shape:\", df.shape)\n",
    "\n",
    "# 3) Compute and print the race distribution\n",
    "counts = df['race'].value_counts(dropna=False)\n",
    "props  = df['race'].value_counts(dropna=False, normalize=True) * 100\n",
    "\n",
    "print('\\nRace Distribution (Counts):')\n",
    "print(counts)\n",
    "print('\\nRace Distribution (Percentage):')\n",
    "print(props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "588589da-4095-4489-9d5d-8751ca565a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Distribution (dem_female) (Counts):\n",
      " dem_female\n",
      "1    30763\n",
      "0    18021\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Gender Distribution (dem_female) (Percentage):\n",
      " dem_female\n",
      "1    63.05961\n",
      "0    36.94039\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Gender Distribution (dem_female) (Counts):\\n', df['dem_female'].value_counts(dropna=False)); print('\\nGender Distribution (dem_female) (Percentage):\\n', df['dem_female'].value_counts(dropna=False, normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7afa80-9259-486d-be58-c192b0f8fcf4",
   "metadata": {},
   "source": [
    "it correctly outputs the code that its using itself to understand the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "387220c2-43b4-42f0-b998-b789093ee26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Band Distribution (Counts):\n",
      " dem_age_band_45-54_tm1    11665\n",
      "dem_age_band_55-64_tm1     9590\n",
      "dem_age_band_35-44_tm1     9481\n",
      "dem_age_band_65-74_tm1     6940\n",
      "dem_age_band_25-34_tm1     5380\n",
      "dem_age_band_75+_tm1       3429\n",
      "dem_age_band_18-24_tm1     1799\n",
      "dtype: int64\n",
      "\n",
      "Age Band Distribution (Percentage):\n",
      " dem_age_band_45-54_tm1    23.911528\n",
      "dem_age_band_55-64_tm1    19.658085\n",
      "dem_age_band_35-44_tm1    19.434651\n",
      "dem_age_band_65-74_tm1    14.225976\n",
      "dem_age_band_25-34_tm1    11.028206\n",
      "dem_age_band_75+_tm1       7.028944\n",
      "dem_age_band_18-24_tm1     3.687684\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "age_cols = [col for col in df.columns if 'dem_age_band_' in col]; age_distribution = df[age_cols].sum().sort_values(ascending=False); print('Age Band Distribution (Counts):\\n', age_distribution); print('\\nAge Band Distribution (Percentage):\\n', (age_distribution / df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e849459d-b26e-41c7-8f1a-b049992fc831",
   "metadata": {},
   "source": [
    "### Comment:\n",
    "outputs similar to manual audit, did not need to adjust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2541f-847c-41e2-941e-e8dae9606af6",
   "metadata": {},
   "source": [
    "### Code below is illustrating reweighting for race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc75e39e-76a0-4e6c-869b-631cae7cee65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m    100\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    102\u001b[0m     X_train_processed,\n\u001b[1;32m    103\u001b[0m     y_train,\n\u001b[1;32m    104\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39mcombined_sample_weights\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#evaluate and check fairness\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, roc_auc_score\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    346\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   1109\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[1;32m   1110\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1111\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m   1112\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   1113\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[1;32m   1114\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[1;32m   1115\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[1;32m   1116\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[1;32m   1117\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[1;32m   1118\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[1;32m    922\u001b[0m             array,\n\u001b[1;32m    923\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m         )\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "#Old code provided by LLM:\n",
    "#import numpy as np\n",
    "#from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Assume X_train_df is your training DataFrame before final processing to numpy\n",
    "# and it contains the original 'race' column.\n",
    "# y_train is your training target Series.\n",
    "\n",
    "# Calculate weights for race (inverse of proportion)\n",
    "#race_counts = X_train_df['race'].value_counts()\n",
    "#race_weights_map = {level: len(X_train_df) / count for level, count in race_counts.items()}\n",
    "#sample_weights_for_race = X_train_df['race'].map(race_weights_map).fillna(1.0).to_numpy()\n",
    "\n",
    "# If your target y_train is also imbalanced, you might want to use class_weight in the model\n",
    "# OR combine sample_weights_for_race with sample weights for the target.\n",
    "# For example, if also using compute_sample_weight for the target:\n",
    "# sample_weights_for_target = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "# combined_sample_weights = sample_weights_for_race * sample_weights_for_target # Element-wise product\n",
    "\n",
    "# # Fit model using these weights\n",
    "# # model = RandomForestClassifier(random_state=42) # Or your chosen model\n",
    "# # model.fit(X_train_processed, y_train, sample_weight=combined_sample_weights) # or sample_weights_for_race\n",
    "\n",
    "# # ---- Evaluation ----\n",
    "# # After training, predict on X_test_processed (which was not re-weighted or re-sampled)\n",
    "# # y_pred = model.predict(X_test_processed)\n",
    "# # Then, evaluate fairness metrics by comparing y_pred to y_test,\n",
    "# # disaggregated by the original 'race', 'dem_female', 'dem_age_band' values in X_test_orig.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Read in your CSV and dictionary\n",
    "df = pd.read_csv(\"data_new.csv\")\n",
    "\n",
    "# 2. Choose your target and features\n",
    "y = df[\"program_enrolled_t\"]     # e.g. outcome at time t\n",
    "# e.g. drop all time-t outcomes and identifiers to form X\n",
    "drop_cols = [\"program_enrolled_t\", \"cost_t\", \"cost_avoidable_t\", \n",
    "             \"risk_score_t\", \"gagne_sum_t\"]\n",
    "X = df.drop(columns=drop_cols)\n",
    "\n",
    "# 3. Hold out a test set (stratify on race so that test/train both\n",
    "#    reflect the same race distribution)\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"race\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "#Preproces features\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# A. Weight by race: inverse of each group’s size\n",
    "race_counts = X_train_df[\"race\"].value_counts()\n",
    "race_weights_map = {\n",
    "    level: len(X_train_df) / count \n",
    "    for level, count in race_counts.items()\n",
    "}\n",
    "sample_weights_race = (\n",
    "    X_train_df[\"race\"]\n",
    "    .map(race_weights_map)\n",
    "    .fillna(1.0)\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "# B. (Optional) Weight by target class imbalance\n",
    "sample_weights_target = compute_sample_weight(\n",
    "    class_weight=\"balanced\", \n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# C. Combine them (element-wise)\n",
    "combined_sample_weights = sample_weights_race * sample_weights_target\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Identify columns\n",
    "cat_cols = [\"race\", \"dem_female\"]  # etc.\n",
    "num_cols = [c for c in X_train_df if c not in cat_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"onehot\", OneHotEncoder(drop=\"if_binary\"), cat_cols),\n",
    "    (\"scale\", StandardScaler(), num_cols),\n",
    "])\n",
    "\n",
    "# Fit on train, transform train & test\n",
    "X_train_processed = preprocessor.fit_transform(X_train_df)\n",
    "X_test_processed  = preprocessor.transform(X_test_df)\n",
    "\n",
    "#fit your model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(\n",
    "    X_train_processed,\n",
    "    y_train,\n",
    "    sample_weight=combined_sample_weights\n",
    ")\n",
    "\n",
    "\n",
    "#evaluate and check fairness\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Predictions on the untouched test set\n",
    "y_pred  = model.predict(X_test_processed)\n",
    "y_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test, y_pred))\n",
    "print(\"ROC-AUC:\",    roc_auc_score(y_test, y_proba))\n",
    "\n",
    "# Then disaggregate by race, gender, age_band, etc.,\n",
    "# to compute error rates or other fairness metrics per group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6cac08-8310-475a-92e2-152d9e635e1f",
   "metadata": {},
   "source": [
    "### comment\n",
    "I changed:\n",
    "Added data‐loading & train/test split block (the original code acted like it was already splitted)\n",
    "New variables introduced (8): df, y, drop_cols, X, X_train_df, X_test_df, y_train, y_test.\n",
    "\n",
    "changed names variables:\n",
    "race_counts           # identical\n",
    "race_weights_map      # identical\n",
    "sample_weights_race   # renamed (dropped “_for_”)\n",
    "renamedvariable: sample_weights_for_target\n",
    "added a preprocessing pipeline because Scikit-learn models (e.g. RandomForest, LogisticRegression) require numerical input arrays. The raw X_train_df likely contains categorical columns (race, dem_female, dem_age_band, …) plus numeric ones.\n",
    "The pipeline with OneHotEncoder and StandardScaler does two things in one go: Encodes categories into 0/1 dummy features\n",
    "and Scales continuous features to zero mean/unit variance\n",
    "\n",
    "added lines for evaluation:\n",
    "to see if re-weighting helped or hurt, nor could you disaggregate errors by race, gender, age, etc., to actually assess fairness\n",
    "The original pointed you toward group-wise fairness metrics (“disaggregated by race, gender, age”), but didn’t show you how to compute any baseline performance numbers.\n",
    "\n",
    "code got an eror because randomforesclassifier does not accept missing values encoded as NaN natively. Or use regressor or change the preprocessing pipeline so that any NaNs get imputed before encoding or scaling. in the original code randoforestclassifier was also mentioned but the missing values as NaN were appearantly not considered. \n",
    "\n",
    "## Below is the improved code: (run it again to see if it works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97674194-a6a5-418d-b787-a311cbcb6cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9899559290765604\n",
      "ROC-AUC: 0.8219044942751922\n",
      "    race  dem_female  n_samples  accuracy   roc_auc  fpr  fnr\n",
      "0  black           0      352.0  0.982955  0.953516  0.0  1.0\n",
      "1  black           1      764.0  0.986911  0.819363  0.0  1.0\n",
      "2  white           0     3233.0  0.989793  0.831899  0.0  1.0\n",
      "3  white           1     5408.0  0.990939  0.798346  0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv(\"../data_new.csv\")\n",
    "\n",
    "# 2. Define target and features\n",
    "y = df[\"program_enrolled_t\"]\n",
    "drop_cols = [\"program_enrolled_t\", \"cost_t\", \"cost_avoidable_t\", \"risk_score_t\", \"gagne_sum_t\"]\n",
    "X = df.drop(columns=drop_cols)\n",
    "\n",
    "# 3. Split into train and test (stratified on race)\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"race\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Compute sample weights for fairness\n",
    "# 4A: Race-based weights (inverse of group size)\n",
    "race_counts = X_train_df[\"race\"].value_counts()\n",
    "race_weights_map = {lvl: len(X_train_df) / cnt for lvl, cnt in race_counts.items()}\n",
    "sample_weights_race = X_train_df[\"race\"].map(race_weights_map).fillna(1.0).to_numpy()\n",
    "\n",
    "# 4B: Target-class weights (balanced)\n",
    "sample_weights_target = compute_sample_weight(class_weight=\"balanced\", y=y_train)\n",
    "\n",
    "# 4C: Combine weights\n",
    "combined_sample_weights = sample_weights_race * sample_weights_target\n",
    "\n",
    "# 5. Build preprocessing pipeline (impute → encode/scale)\n",
    "cat_cols = [\"race\", \"dem_female\"]\n",
    "num_cols = [c for c in X_train_df.columns if c not in cat_cols]\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),       # fill NaN with mode\n",
    "    (\"onehot\", OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),                # fill NaN with mean\n",
    "    (\"scale\",   StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", cat_pipeline, cat_cols),\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "])\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train_df)\n",
    "X_test_processed  = preprocessor.transform(X_test_df)\n",
    "\n",
    "# 6. Train the model with sample weights\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(\n",
    "    X_train_processed,\n",
    "    y_train,\n",
    "    sample_weight=combined_sample_weights\n",
    ")\n",
    "\n",
    "# 7. Evaluate overall performance\n",
    "y_pred  = model.predict(X_test_processed)\n",
    "y_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "print(\"Accuracy:\",  accuracy_score(y_test, y_pred))\n",
    "print(\"ROC-AUC:\",   roc_auc_score(y_test, y_proba))\n",
    "\n",
    "# 8. (Next) Disaggregate results by race/gender/age for fairness metrics:\n",
    "#    e.g. for each group in X_test_df[cat_cols], compute false-positive/negative rates, AUC, etc.\n",
    "# 8A. Build a results DataFrame\n",
    "results = X_test_df.copy()\n",
    "results[\"y_true\"]    = y_test.values\n",
    "results[\"y_pred\"]    = y_pred\n",
    "results[\"y_proba\"]   = y_proba\n",
    "\n",
    "# (Optional) if you have an age‐group column, e.g. 'age_group', include it in cat_cols\n",
    "group_cols = [\"race\", \"dem_female\"]  # add \"age_group\" here if available\n",
    "\n",
    "# 8B. Define a helper to compute FPR / FNR\n",
    "def group_metrics(df):\n",
    "    tn, fp, fn, tp = confusion_matrix(df[\"y_true\"], df[\"y_pred\"], labels=[0,1]).ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    roc_auc  = roc_auc_score(df[\"y_true\"], df[\"y_proba\"]) if len(df[\"y_true\"].unique()) > 1 else np.nan\n",
    "    return pd.Series({\n",
    "        \"n_samples\": len(df),\n",
    "        \"accuracy\":  accuracy,\n",
    "        \"roc_auc\":   roc_auc,\n",
    "        \"fpr\":       fp / (fp + tn) if (fp + tn) > 0 else np.nan,\n",
    "        \"fnr\":       fn / (fn + tp) if (fn + tp) > 0 else np.nan\n",
    "    })\n",
    "\n",
    "# 8C. Apply group_metrics to each subgroup\n",
    "fairness_table = (\n",
    "    results\n",
    "    .groupby(group_cols)\n",
    "    .apply(group_metrics)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(fairness_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24876612-2c0d-4d13-9338-1f3f273a9fa6",
   "metadata": {},
   "source": [
    "## Output\n",
    "An overall accuracy of 98.99 percent and a ROC AUC of 0.82 indicate strong aggregate performance. However, every subgroup exhibits a 0 percent false positive rate and a 100 percent false negative rate, which means the model never predicts any enrollments at the default threshold and thus inflates accuracy in the face of class imbalance. The ROC AUC values, ranging from 0.80 to 0.95, show that the predicted probabilities do separate the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fcaf5e-25e0-45f2-af16-b183e7ea9d8f",
   "metadata": {},
   "source": [
    "# Prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "067e3410-a228-430a-956f-6c161f0c3f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program Enrolled (program_enrolled_t) Distribution (Counts):\n",
      " program_enrolled_t\n",
      "0    48332\n",
      "1      452\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Program Enrolled (program_enrolled_t) Distribution (Percentage):\n",
      " program_enrolled_t\n",
      "0    99.073467\n",
      "1     0.926533\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd; df = pd.read_csv('data_new.csv'); print('Program Enrolled (program_enrolled_t) Distribution (Counts):\\n', df['program_enrolled_t'].value_counts(dropna=False)); print('\\nProgram Enrolled (program_enrolled_t) Distribution (Percentage):\\n', df['program_enrolled_t'].value_counts(dropna=False, normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bae4915d-97fb-4a51-a864-40be6b89e5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program Enrollment Rate by Race:\n",
      "\n",
      "program_enrolled_t          0         1\n",
      "race                                   \n",
      "black               98.584737  1.415263\n",
      "white               99.136614  0.863386\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd; df = pd.read_csv('data_new.csv'); print('Program Enrollment Rate by Race:\\n'); print(pd.crosstab(df['race'], df['program_enrolled_t'], normalize='index') * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2122f061-d7a6-411d-b11b-79d89483fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program Enrollment Rate by Gender (dem_female):\n",
      "\n",
      "program_enrolled_t          0         1\n",
      "dem_female                             \n",
      "0                   98.945674  1.054326\n",
      "1                   99.148328  0.851672\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd; df = pd.read_csv('data_new.csv'); print('Program Enrollment Rate by Gender (dem_female):\\n'); print(pd.crosstab(df['dem_female'], df['program_enrolled_t'], normalize='index') * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbbfb0f9-34ea-47f8-b416-6e23a2307f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program Enrollment Rate by Age Group:\n",
      "\n",
      "program_enrolled_t              0         1\n",
      "age_group                                  \n",
      "dem_age_band_18-24_tm1  99.705666  0.294334\n",
      "dem_age_band_25-34_tm1  99.773542  0.226458\n",
      "dem_age_band_35-44_tm1  99.552143  0.447857\n",
      "dem_age_band_45-54_tm1  99.346461  0.653539\n",
      "dem_age_band_55-64_tm1  98.915537  1.084463\n",
      "dem_age_band_65-74_tm1  98.160584  1.839416\n",
      "dem_age_band_75+_tm1    97.469880  2.530120\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd; df = pd.read_csv('data_new.csv'); age_cols = [col for col in df.columns if 'dem_age_band_' in col]; df['age_group'] = df[age_cols].idxmax(axis=1); print('Program Enrollment Rate by Age Group:\\n'); print(pd.crosstab(df['age_group'], df['program_enrolled_t'], normalize='index') * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1776b-117b-4ecd-930e-ff18790eaff0",
   "metadata": {},
   "source": [
    "### comment \n",
    "The code that the llm uses and after it gives the output of the code is correct untill now. After running the code that\n",
    "the llm used in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4327cf-1e49-42f5-869d-3c99942cdb27",
   "metadata": {},
   "source": [
    "# Prompt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba1a5c-40a4-402d-b5b5-57cd222bbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conceptual Code Snippets ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression # Example model\n",
    "from collections import Counter\n",
    "\n",
    "# For imblearn (if not installed: pip install imbalanced-learn)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline # Pipeline that can include samplers\n",
    "\n",
    "# --- Assume initial data loading ---\n",
    "# df = pd.read_csv('data_new.csv')\n",
    "# y = df['program_enrolled_t']\n",
    "# X = df.drop('program_enrolled_t', axis=1)\n",
    "\n",
    "# # --- 1. Train-Test Split (Stratified) ---\n",
    "# X_train_orig, X_test_orig, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "#\n",
    "# # Create copies for processing\n",
    "# X_train = X_train_orig.copy()\n",
    "# X_test = X_test_orig.copy()\n",
    "\n",
    "# --- 2. Preprocessing (Illustrative - adapt to your 160 columns) ---\n",
    "# Identify numerical and categorical columns (example subset)\n",
    "# numerical_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "# categorical_cols = ['race'] # Add other categoricals\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical data\n",
    "# numerical_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "#\n",
    "# categorical_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')), # or 'constant', fill_value='Missing'\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "# ])\n",
    "\n",
    "# Combine preprocessing pipelines using ColumnTransformer\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numerical_pipeline, numerical_cols),\n",
    "#         ('cat', categorical_pipeline, categorical_cols)\n",
    "#     ],\n",
    "#     remainder='passthrough' # Or 'drop' if unlisted columns are not needed\n",
    "# )\n",
    "\n",
    "# --- 3. Model Training with Strategies ---\n",
    "\n",
    "# Strategy A: Cost-Sensitive Learning for Target Imbalance\n",
    "# model_cost_sensitive = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42)\n",
    "# pipeline_cost_sensitive = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', model_cost_sensitive)\n",
    "# ])\n",
    "# # pipeline_cost_sensitive.fit(X_train, y_train)\n",
    "# print(f\"Training with Cost-Sensitive Learning for y. Original y_train counts: {Counter(y_train)}\")\n",
    "\n",
    "# Strategy B: SMOTE for Target Imbalance (using imblearn's Pipeline)\n",
    "# model_smote = LogisticRegression(solver='liblinear', random_state=42) # No class_weight here\n",
    "# pipeline_smote = ImbPipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('sampler', SMOTE(random_state=42, sampling_strategy='auto')), # Default is to oversample minority\n",
    "#     ('classifier', model_smote)\n",
    "# ])\n",
    "# # pipeline_smote.fit(X_train, y_train)\n",
    "# # Access post-SMOTE counts conceptually (sampler is part of pipeline)\n",
    "# # X_train_processed_temp = preprocessor.fit_transform(X_train)\n",
    "# # y_train_smote_temp = SMOTE(random_state=42).fit_resample(X_train_processed_temp, y_train)[1]\n",
    "# # print(f\"Training with SMOTE for y. y_train counts after SMOTE (conceptual): {Counter(y_train_smote_temp)}\")\n",
    "\n",
    "\n",
    "# Strategy C: Sample Re-weighting for Feature Imbalance (e.g., 'race')\n",
    "# This is applied via sample_weight in model.fit()\n",
    "# model_reweighted = LogisticRegression(solver='liblinear', random_state=42) # Potentially also class_weight='balanced' for target\n",
    "# Fit preprocessor first\n",
    "# X_train_processed = preprocessor.fit_transform(X_train)\n",
    "# X_test_processed = preprocessor.transform(X_test) # Transform test set too\n",
    "\n",
    "# Create sample weights (conceptual - based on original X_train before processing if needed for feature value)\n",
    "# Assuming 'race' column is still accessible or mapped from preprocessed data\n",
    "# race_weights_map = {'white': 1.0, 'black': len(X_train) / X_train['race'].value_counts()['black']} # Example weights\n",
    "# sample_weights = X_train['race'].map(race_weights_map).fillna(1.0).to_numpy()\n",
    "#\n",
    "# # If using class_weight='balanced' with sample_weight, the effects multiply.\n",
    "# # model_reweighted.fit(X_train_processed, y_train, sample_weight=sample_weights)\n",
    "# print(f\"Training with sample re-weighting for 'race'. (Example weights: {race_weights_map})\")\n",
    "\n",
    "\n",
    "# --- 4. Evaluation (on X_test_processed, y_test) ---\n",
    "# For each trained pipeline/model:\n",
    "# y_pred = pipeline_X.predict(X_test)\n",
    "# y_proba = pipeline_X.predict_proba(X_test)[:, 1]\n",
    "#\n",
    "# Calculate overall performance (AUC, F1 for 'Enrolled', etc.)\n",
    "# Calculate fairness metrics (Demographic Parity, Equal Opportunity, etc.)\n",
    "# by comparing y_pred/y_proba to y_test, disaggregated by columns in X_test_orig\n",
    "# (e.g., X_test_orig['race'], X_test_orig['dem_female'], etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b7dd02-8532-42fd-a168-637b6c654061",
   "metadata": {},
   "source": [
    "## comment\n",
    "adapt the loading/splitting lines (define df, X, y).\n",
    "\n",
    "Define numerical_cols (e.g. df.select_dtypes(include=np.number)) and list your categorical columns (e.g. ['race', 'dem_female', …]).\n",
    "\n",
    "Install and import imbalanced-learn (SMOTE, ImbPipeline).\n",
    "\n",
    "Instantiate and fit each pipeline/model (you’ll need to call .fit() and then .predict() or .predict_proba() for each strategy).\n",
    "\n",
    "Compute the actual fairness metrics—e.g. false-positive rates per group—after predicting on your held-out test set.\n",
    "\n",
    "Handle any missing values or encoding issues exactly as in your working pipeline.\n",
    "\n",
    "## Below the new code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f14d9f0-1ac7-4c09-a32b-9542e40b8929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cost‐Sensitive ===\n",
      "y_train counts: Counter({0: 36249, 1: 339})\n",
      "Accuracy: 0.8510987208920958\n",
      "ROC-AUC: 0.8855819519708445\n",
      "\n",
      "=== SMOTE Oversampling ===\n",
      "y_train counts before SMOTE: Counter({0: 36249, 1: 339})\n",
      "Accuracy: 0.8415874057067891\n",
      "ROC-AUC: 0.8644757243226973\n",
      "\n",
      "=== Sample Re‐weighting (race) ===\n",
      "race counts: {'white': 32391, 'black': 4197}\n",
      "Sample weights map: {'white': 1.129573029545244, 'black': 8.717655468191566}\n",
      "Accuracy: 0.9897507379468679\n",
      "ROC-AUC: 0.8072007845440717\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv(\"data_new.csv\")\n",
    "\n",
    "# 2. Define target and features\n",
    "y = df[\"program_enrolled_t\"]                     # outcome at time t :contentReference[oaicite:1]{index=1}\n",
    "X = df.drop(columns=[\"program_enrolled_t\"])\n",
    "\n",
    "# 3. Train/test split (stratify on y for balanced classes)\n",
    "X_train_orig, X_test_orig, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Working copies\n",
    "X_train = X_train_orig.copy()\n",
    "X_test  = X_test_orig.copy()\n",
    "\n",
    "# 4. Identify numerical and categorical columns\n",
    "numerical_cols   = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_cols = [\"race\", \"dem_female\"]        # both exist in data_new.csv :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "# 5. Preprocessing pipelines\n",
    "numerical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\",   StandardScaler())\n",
    "])\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\",  OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_pipeline,   numerical_cols),\n",
    "        (\"cat\", categorical_pipeline, categorical_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# 6A. Strategy A: Cost‐Sensitive Learning for y imbalance\n",
    "model_cost = LogisticRegression(\n",
    "    solver=\"liblinear\",\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "pipeline_cost = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\",   model_cost)\n",
    "])\n",
    "pipeline_cost.fit(X_train, y_train)\n",
    "y_pred_cost  = pipeline_cost.predict(X_test)\n",
    "y_proba_cost = pipeline_cost.predict_proba(X_test)[:, 1]\n",
    "print(\"Cost‐Sensitive\")\n",
    "print(\"y_train counts:\", Counter(y_train))\n",
    "print(\"Accuracy:\",  accuracy_score(y_test, y_pred_cost))\n",
    "print(\"ROC-AUC:\",   roc_auc_score(y_test, y_proba_cost))\n",
    "print()\n",
    "\n",
    "# 6B. Strategy B: SMOTE Oversampling for y imbalance\n",
    "model_smote = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "pipeline_smote = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"smote\",        SMOTE(random_state=42, sampling_strategy=\"auto\")),\n",
    "    (\"classifier\",   model_smote)\n",
    "])\n",
    "pipeline_smote.fit(X_train, y_train)\n",
    "y_pred_smote  = pipeline_smote.predict(X_test)\n",
    "y_proba_smote = pipeline_smote.predict_proba(X_test)[:, 1]\n",
    "print(\"SMOTE Oversampling\")\n",
    "print(\"y_train counts before SMOTE:\", Counter(y_train))\n",
    "print(\"Accuracy:\",  accuracy_score(y_test, y_pred_smote))\n",
    "print(\"ROC-AUC:\",   roc_auc_score(y_test, y_proba_smote))\n",
    "print()\n",
    "\n",
    "# 6C. Strategy C: Sample Re‐weighting for 'race'\n",
    "race_counts    = X_train[\"race\"].value_counts()                  # white/black :contentReference[oaicite:3]{index=3}\n",
    "race_weights   = {lvl: len(X_train) / cnt for lvl, cnt in race_counts.items()}\n",
    "sample_weights = X_train[\"race\"].map(race_weights).fillna(1.0).to_numpy()\n",
    "\n",
    "model_reweight = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "pipeline_rew = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\",   model_reweight)\n",
    "])\n",
    "pipeline_rew.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    classifier__sample_weight=sample_weights\n",
    ")\n",
    "y_pred_rew  = pipeline_rew.predict(X_test)\n",
    "y_proba_rew = pipeline_rew.predict_proba(X_test)[:, 1]\n",
    "print(\"Sample Re‐weighting (race)\")\n",
    "print(\"race counts:\", race_counts.to_dict())\n",
    "print(\"Sample weights map:\", race_weights)\n",
    "print(\"Accuracy:\",  accuracy_score(y_test, y_pred_rew))\n",
    "print(\"ROC-AUC:\",   roc_auc_score(y_test, y_proba_rew))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44750e-47d8-4ec7-978b-a3eeee7b2bc0",
   "metadata": {},
   "source": [
    "## Output:\n",
    "The cost sensitive approach which heavily penalizes misclassifying the minority class reduced overall accuracy to about 85.1 percent and yielded a ROC AUC of 0.886. SMOTE oversampling produced similar effects, with accuracy of 84.2 percent and ROC AUC of 0.864 as synthetic minority examples encouraged the model to predict more positives. Finally, reweighting samples by race so that each white case carried a weight of approximately 1.13 and each black case a weight of about 8.72 restored very high accuracy of 98.98 percent while maintaining a ROC AUC of 0.807. This shows that placing greater emphasis on under represented groups can preserve the model’s ability to distinguish true positives from true negatives without undermining its overall discrimination performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
